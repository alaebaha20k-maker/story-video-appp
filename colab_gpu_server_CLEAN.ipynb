{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ¬ Professional AI Video Generator - GPU Server\n",
    "## Complete System with Coqui TTS + DreamShaper XL\n",
    "\n",
    "### âš ï¸ IMPORTANT INSTRUCTIONS:\n",
    "1. **Enable GPU**: Runtime â†’ Change runtime type â†’ T4 GPU â†’ Save\n",
    "2. **Run Cell 1** â†’ Installs Python 3.10 environment (runtime restarts)\n",
    "3. **Run Cell 1 AGAIN** â†’ Installs all dependencies\n",
    "4. **Run Cell 2, 3, 4...** â†’ Everything will work!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# ğŸ“¦ CELL 1: INSTALL ALL DEPENDENCIES\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# âš ï¸ RUN THIS CELL TWICE:\n",
    "#    First run: Installs Python 3.10, restarts runtime\n",
    "#    Second run (after restart): Installs all packages\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "import sys\n",
    "\n",
    "print(f\"ğŸ“ Current Python: {sys.version}\\n\")\n",
    "\n",
    "if sys.version_info >= (3, 12):\n",
    "    print(\"âš ï¸  Python 3.12 detected - Coqui TTS needs Python 3.10!\")\n",
    "    print(\"ğŸ”„ Installing Python 3.10 environment...\\n\")\n",
    "    \n",
    "    !pip install -q condacolab\n",
    "    import condacolab\n",
    "    condacolab.install()\n",
    "    \n",
    "    print(\"\\nâœ… Python 3.10 environment installed!\")\n",
    "    print(\"âš ï¸  Runtime will restart automatically...\")\n",
    "    print(\"âš ï¸  AFTER RESTART: Run this cell again to install packages!\")\n",
    "    \n",
    "else:\n",
    "    print(f\"âœ… Python {sys.version_info.major}.{sys.version_info.minor} - Compatible!\\n\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"ğŸ“¦ INSTALLING ALL DEPENDENCIES\")\n",
    "    print(\"=\"*80 + \"\\n\")\n",
    "    \n",
    "    # FFmpeg\n",
    "    print(\"ğŸ¬ Installing FFmpeg...\")\n",
    "    !apt-get update -qq > /dev/null 2>&1\n",
    "    !apt-get install -y -qq ffmpeg > /dev/null 2>&1\n",
    "    !ffmpeg -version | head -n 1\n",
    "    print(\"   âœ… FFmpeg installed!\\n\")\n",
    "    \n",
    "    # Core packages\n",
    "    print(\"ğŸ“¦ Installing Flask and core packages...\")\n",
    "    !pip install -q --upgrade pip\n",
    "    !pip install -q flask flask-cors pyngrok requests\n",
    "    print(\"   âœ… Flask packages installed!\\n\")\n",
    "    \n",
    "    # PyTorch\n",
    "    print(\"ğŸ”¥ Installing PyTorch (GPU)...\")\n",
    "    !pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "    print(\"   âœ… PyTorch installed!\\n\")\n",
    "    \n",
    "    # Coqui TTS\n",
    "    print(\"ğŸ¤ Installing Coqui TTS...\")\n",
    "    !pip install -q TTS soundfile numpy scipy\n",
    "    print(\"   âœ… Coqui TTS installed!\\n\")\n",
    "    \n",
    "    # DreamShaper XL\n",
    "    print(\"ğŸ¨ Installing DreamShaper XL...\")\n",
    "    !pip install -q diffusers transformers accelerate safetensors\n",
    "    print(\"   âœ… DreamShaper XL installed!\\n\")\n",
    "    \n",
    "    # Image tools\n",
    "    print(\"ğŸ“¸ Installing image tools...\")\n",
    "    !pip install -q pillow opencv-python-headless\n",
    "    print(\"   âœ… Image tools installed!\\n\")\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"âœ… ALL DEPENDENCIES INSTALLED SUCCESSFULLY!\")\n",
    "    print(\"=\"*80)\n",
    "    print(\"\\nğŸ‘‰ Now run Cell 2 to setup GPU and imports!\")\n",
    "    print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# ğŸ”§ CELL 2: SETUP GPU & IMPORTS\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "import gc\n",
    "import torch\n",
    "import json\n",
    "import subprocess\n",
    "import base64\n",
    "import time\n",
    "import io\n",
    "import re\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "from flask import Flask, request, jsonify, send_file\n",
    "from flask_cors import CORS\n",
    "from pyngrok import ngrok\n",
    "from threading import Thread\n",
    "\n",
    "# GPU Detection\n",
    "print(\"=\"*80)\n",
    "print(\"ğŸ” GPU DETECTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "    print(f\"âœ… GPU ENABLED: {gpu_name}\")\n",
    "    print(f\"ğŸ’¾ GPU Memory: {gpu_memory:.1f} GB\")\n",
    "    print(f\"ğŸ”¥ CUDA Version: {torch.version.cuda}\")\n",
    "else:\n",
    "    device = 'cpu'\n",
    "    print(\"âš ï¸  WARNING: GPU NOT DETECTED\")\n",
    "\n",
    "print(f\"\\nğŸš€ Device: {device}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create output directory\n",
    "output_dir = Path('/content/outputs')\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"\\nğŸ“ Output directory: {output_dir}\")\n",
    "print(\"\\nâœ… Setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# ğŸ¨ CELL 3: LOAD IMAGE GENERATION MODEL (DreamShaper XL)\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "from diffusers import StableDiffusionXLPipeline\n",
    "\n",
    "# Global model variable\n",
    "img_pipeline = None\n",
    "\n",
    "# Style definitions for all 12 styles\n",
    "STYLE_DEFINITIONS = {\n",
    "    \"cinematic\": {\n",
    "        \"positive\": \"cinematic film still, movie quality, professional cinematography, depth of field, bokeh, film grain, dramatic lighting\",\n",
    "        \"negative\": \"amateur, low quality, blurry, cartoon, anime\"\n",
    "    },\n",
    "    \"documentary\": {\n",
    "        \"positive\": \"documentary photography, National Geographic style, photojournalism, natural lighting, authentic, real life\",\n",
    "        \"negative\": \"staged, fake, cartoon, anime, low quality\"\n",
    "    },\n",
    "    \"anime\": {\n",
    "        \"positive\": \"anime style, manga illustration, cel shaded, vibrant colors, Japanese animation, studio ghibli quality\",\n",
    "        \"negative\": \"photorealistic, 3D render, western cartoon, low quality\"\n",
    "    },\n",
    "    \"horror\": {\n",
    "        \"positive\": \"horror atmosphere, dark and creepy, ominous lighting, terrifying, nightmare fuel, psychological horror, eerie\",\n",
    "        \"negative\": \"bright, cheerful, colorful, cute, low quality\"\n",
    "    },\n",
    "    \"comic\": {\n",
    "        \"positive\": \"comic book art, graphic novel style, bold lines, dynamic composition, pop art colors, marvel comics style\",\n",
    "        \"negative\": \"photorealistic, blurry, low quality, amateur\"\n",
    "    },\n",
    "    \"historical\": {\n",
    "        \"positive\": \"historical photograph, vintage photo, sepia tone, aged photograph, 1920s style, grainy, archival quality\",\n",
    "        \"negative\": \"modern, digital, colorful, cartoon, low quality\"\n",
    "    },\n",
    "    \"scifi\": {\n",
    "        \"positive\": \"sci-fi concept art, cyberpunk, neon lights, futuristic technology, blade runner style, advanced technology\",\n",
    "        \"negative\": \"historical, vintage, low quality, blurry\"\n",
    "    },\n",
    "    \"noir\": {\n",
    "        \"positive\": \"film noir, high contrast black and white, dramatic shadows, 1940s detective aesthetic, cinematic\",\n",
    "        \"negative\": \"colorful, bright, cartoon, low quality\"\n",
    "    },\n",
    "    \"fantasy\": {\n",
    "        \"positive\": \"epic fantasy art, magical atmosphere, dungeons and dragons style, concept art, detailed, enchanted\",\n",
    "        \"negative\": \"modern, realistic, photograph, low quality\"\n",
    "    },\n",
    "    \"3d\": {\n",
    "        \"positive\": \"3D render, Pixar style, CGI graphics, Blender render, octane render, photorealistic 3D\",\n",
    "        \"negative\": \"2D, flat, painting, sketch, low quality\"\n",
    "    },\n",
    "    \"sketch\": {\n",
    "        \"positive\": \"pencil sketch, hand-drawn, charcoal drawing, line art, traditional art, artistic sketch\",\n",
    "        \"negative\": \"photorealistic, 3D render, colorful, low quality\"\n",
    "    },\n",
    "    \"watercolor\": {\n",
    "        \"positive\": \"watercolor painting, soft edges, flowing colors, artistic, traditional painting, hand-painted\",\n",
    "        \"negative\": \"photorealistic, 3D, digital art, low quality\"\n",
    "    }\n",
    "}\n",
    "\n",
    "def load_image_model():\n",
    "    \"\"\"Load DreamShaper XL model\"\"\"\n",
    "    global img_pipeline\n",
    "    \n",
    "    if img_pipeline is None:\n",
    "        print(\"ğŸ¨ Loading DreamShaper XL (Supports ALL 12 styles!)...\")\n",
    "        \n",
    "        img_pipeline = StableDiffusionXLPipeline.from_pretrained(\n",
    "            \"Lykon/dreamshaper-xl-1-0\",\n",
    "            torch_dtype=torch.float16,\n",
    "            variant=\"fp16\",\n",
    "            use_safetensors=True\n",
    "        ).to(device)\n",
    "        \n",
    "        img_pipeline.enable_xformers_memory_efficient_attention()\n",
    "        \n",
    "        print(\"   âœ… DreamShaper XL loaded!\")\n",
    "        print(\"   ğŸ¯ Supports: cinematic, documentary, anime, horror, comic, etc.\")\n",
    "        print(\"   ğŸ“ Resolution: 1536x864 (16:9 ratio)\")\n",
    "    \n",
    "    return img_pipeline\n",
    "\n",
    "def generate_single_image(prompt, style=\"cinematic\"):\n",
    "    \"\"\"Generate single image with specific style\"\"\"\n",
    "    pipeline = load_image_model()\n",
    "    \n",
    "    # Get style keywords\n",
    "    style_def = STYLE_DEFINITIONS.get(style.lower(), STYLE_DEFINITIONS[\"cinematic\"])\n",
    "    \n",
    "    # Build full prompt\n",
    "    full_prompt = f\"{prompt}, {style_def['positive']}, highly detailed, professional quality, 8k\"\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # Generate\n",
    "    with torch.inference_mode():\n",
    "        image = pipeline(\n",
    "            prompt=full_prompt,\n",
    "            negative_prompt=style_def['negative'],\n",
    "            num_inference_steps=25,\n",
    "            guidance_scale=7.5,\n",
    "            height=864,\n",
    "            width=1536\n",
    "        ).images[0]\n",
    "    \n",
    "    return image\n",
    "\n",
    "print(\"âœ… Image generation functions loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# ğŸ¤ CELL 4: VOICE MAPPING & TTS (Coqui TTS - VCTK Speakers)\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "from TTS.api import TTS\n",
    "import soundfile as sf\n",
    "\n",
    "# Global TTS model\n",
    "tts_model = None\n",
    "\n",
    "# Voice mapping\n",
    "VOICE_MAPPING = {\n",
    "    # Male voices\n",
    "    'guy': 'p226', 'adam': 'p226', 'brian': 'p227', 'andrew': 'p243',\n",
    "    'michael': 'p232', 'george': 'p254', 'christopher': 'p259', 'davis_deep': 'p273',\n",
    "    # Female voices\n",
    "    'aria': 'p229', 'sarah': 'p231', 'nicole': 'p233', 'jenny': 'p228',\n",
    "    'emma': 'p230', 'emma_british': 'p236', 'isabella': 'p244', 'sara': 'p231',\n",
    "    # Defaults\n",
    "    'default_male': 'p226', 'default_female': 'p229',\n",
    "}\n",
    "\n",
    "def get_coqui_speaker(voice_id):\n",
    "    \"\"\"Get Coqui VCTK speaker ID from frontend voice ID\"\"\"\n",
    "    return VOICE_MAPPING.get(voice_id, 'p226')\n",
    "\n",
    "def load_tts_model():\n",
    "    \"\"\"Load Coqui TTS model\"\"\"\n",
    "    global tts_model\n",
    "    \n",
    "    if tts_model is None:\n",
    "        print(\"ğŸ¤ Loading Coqui TTS (PyTorch GPU)...\")\n",
    "        tts_model = TTS(\"tts_models/en/vctk/vits\", progress_bar=False).to(device)\n",
    "        print(\"   âœ… Coqui TTS loaded!\")\n",
    "    \n",
    "    return tts_model\n",
    "\n",
    "def split_text_smart(text, max_chars=1000):\n",
    "    \"\"\"Split text into optimized chunks\"\"\"\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
    "    chunks = []\n",
    "    current_chunk = \"\"\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        if len(current_chunk) + len(sentence) < max_chars:\n",
    "            current_chunk += sentence + \" \"\n",
    "        else:\n",
    "            if current_chunk:\n",
    "                chunks.append(current_chunk.strip())\n",
    "            current_chunk = sentence + \" \"\n",
    "    \n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "    \n",
    "    return chunks if chunks else [text]\n",
    "\n",
    "def generate_audio_parallel(text, voice_id, speed=1.0):\n",
    "    \"\"\"Generate audio with parallel processing\"\"\"\n",
    "    tts = load_tts_model()\n",
    "    speaker = get_coqui_speaker(voice_id)\n",
    "    \n",
    "    chunks = split_text_smart(text, max_chars=1000)\n",
    "    \n",
    "    if len(chunks) == 1:\n",
    "        # Single chunk\n",
    "        temp_file = io.BytesIO()\n",
    "        tts.tts_to_file(text=text, speaker=speaker, file_path=temp_file)\n",
    "        temp_file.seek(0)\n",
    "        audio, sample_rate = sf.read(temp_file)\n",
    "        return audio, sample_rate\n",
    "    \n",
    "    # Parallel processing\n",
    "    def generate_chunk(chunk_text, idx):\n",
    "        temp_file = io.BytesIO()\n",
    "        tts.tts_to_file(text=chunk_text, speaker=speaker, file_path=temp_file)\n",
    "        temp_file.seek(0)\n",
    "        audio, sr = sf.read(temp_file)\n",
    "        return idx, audio, sr\n",
    "    \n",
    "    all_audio = []\n",
    "    sample_rate = None\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        futures = {executor.submit(generate_chunk, chunk, i): i for i, chunk in enumerate(chunks)}\n",
    "        for future in as_completed(futures):\n",
    "            idx, audio, sr = future.result()\n",
    "            all_audio.append((idx, audio))\n",
    "            sample_rate = sr\n",
    "    \n",
    "    all_audio.sort(key=lambda x: x[0])\n",
    "    combined = np.concatenate([audio for _, audio in all_audio])\n",
    "    \n",
    "    if speed != 1.0:\n",
    "        from scipy import signal\n",
    "        samples = int(len(combined) / speed)\n",
    "        combined = signal.resample(combined, samples)\n",
    "    \n",
    "    return combined, sample_rate\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ğŸ¤ VOICE SYSTEM CONFIGURED (COQUI TTS)\")\n",
    "print(\"=\"*80)\n",
    "print(f\"âœ… Total voices: {len(VOICE_MAPPING)}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# ğŸ¬ CELL 5: VIDEO COMPILATION (FFmpeg)\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "def compile_video_mixed_media(media_data, media_types, audio_data, durations, effects, captions=None):\n",
    "    \"\"\"Compile video with unlimited media items + captions\"\"\"\n",
    "    print(f\"ğŸ¬ Compiling video with {len(media_data)} items...\")\n",
    "    \n",
    "    temp_dir = output_dir / \"temp\"\n",
    "    temp_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Save media files\n",
    "    media_paths = []\n",
    "    for i, (data, media_type) in enumerate(zip(media_data, media_types)):\n",
    "        ext = 'png' if media_type == 'image' else 'mp4'\n",
    "        media_path = temp_dir / f\"media_{i:03d}.{ext}\"\n",
    "        \n",
    "        if isinstance(data, str) and data.startswith('data:'):\n",
    "            data = data.split(',')[1]\n",
    "        \n",
    "        media_bytes = base64.b64decode(data)\n",
    "        with open(media_path, 'wb') as f:\n",
    "            f.write(media_bytes)\n",
    "        media_paths.append(media_path)\n",
    "    \n",
    "    # Save audio\n",
    "    audio_path = temp_dir / \"audio.wav\"\n",
    "    if isinstance(audio_data, str) and audio_data.startswith('data:'):\n",
    "        audio_data = audio_data.split(',')[1]\n",
    "    audio_bytes = base64.b64decode(audio_data)\n",
    "    with open(audio_path, 'wb') as f:\n",
    "        f.write(audio_bytes)\n",
    "    \n",
    "    # Process media with effects\n",
    "    processed_paths = []\n",
    "    for i, (media_path, media_type, duration) in enumerate(zip(media_paths, media_types, durations)):\n",
    "        processed_path = temp_dir / f\"processed_{i:03d}.mp4\"\n",
    "        \n",
    "        filters = [\"scale=1920:1080:force_original_aspect_ratio=decrease,pad=1920:1080:(ow-iw)/2:(oh-ih)/2\"]\n",
    "        \n",
    "        if effects.get('zoom_effect', True):\n",
    "            filters.append(\"zoompan=z='min(zoom+0.0015,1.1)':d=1:x=iw/2-(iw/zoom/2):y=ih/2-(ih/zoom/2):s=1920x1080\")\n",
    "        \n",
    "        if effects.get('color_filter') == 'cinematic':\n",
    "            filters.append(\"eq=contrast=1.1:saturation=0.9\")\n",
    "        \n",
    "        if media_type == 'image':\n",
    "            filters.append(\"fps=24\")\n",
    "        \n",
    "        video_filter = ','.join(filters)\n",
    "        \n",
    "        if media_type == 'image':\n",
    "            cmd = ['ffmpeg', '-y', '-loop', '1', '-i', str(media_path), '-t', str(duration),\n",
    "                   '-vf', video_filter, '-c:v', 'libx264', '-preset', 'ultrafast',\n",
    "                   '-pix_fmt', 'yuv420p', str(processed_path)]\n",
    "        else:\n",
    "            cmd = ['ffmpeg', '-y', '-i', str(media_path), '-t', str(duration),\n",
    "                   '-vf', video_filter, '-c:v', 'libx264', '-preset', 'ultrafast',\n",
    "                   '-c:a', 'copy', str(processed_path)]\n",
    "        \n",
    "        subprocess.run(cmd, capture_output=True)\n",
    "        processed_paths.append(processed_path)\n",
    "    \n",
    "    # Concatenate clips\n",
    "    concat_file = temp_dir / \"concat.txt\"\n",
    "    with open(concat_file, 'w') as f:\n",
    "        for path in processed_paths:\n",
    "            f.write(f\"file '{path}'\\n\")\n",
    "    \n",
    "    temp_video_path = temp_dir / \"temp_video.mp4\"\n",
    "    subprocess.run(['ffmpeg', '-y', '-f', 'concat', '-safe', '0', '-i', str(concat_file),\n",
    "                    '-c:v', 'copy', str(temp_video_path)], capture_output=True)\n",
    "    \n",
    "    # Add audio\n",
    "    output_file = output_dir / \"final_video.mp4\"\n",
    "    subprocess.run(['ffmpeg', '-y', '-i', str(temp_video_path), '-i', str(audio_path),\n",
    "                    '-c:v', 'copy', '-c:a', 'aac', '-b:a', '192k',\n",
    "                    '-shortest', str(output_file)], capture_output=True)\n",
    "    \n",
    "    # Cleanup\n",
    "    for path in media_paths + processed_paths + [concat_file, audio_path, temp_video_path]:\n",
    "        if Path(path).exists():\n",
    "            Path(path).unlink()\n",
    "    \n",
    "    print(f\"   âœ… Video compiled: {output_file.name}\")\n",
    "    return output_file\n",
    "\n",
    "print(\"âœ… Video compilation functions loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# ğŸš€ CELL 6: FLASK SERVER & ALL ROUTES\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "app = Flask(__name__)\n",
    "CORS(app)\n",
    "\n",
    "@app.route('/health', methods=['GET'])\n",
    "def health():\n",
    "    return jsonify({\n",
    "        'status': 'healthy',\n",
    "        'device': device,\n",
    "        'gpu': torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None',\n",
    "        'features': {'voices': len(VOICE_MAPPING), 'coqui_tts': True, 'dreamshaper_xl': True}\n",
    "    })\n",
    "\n",
    "@app.route('/generate_audio', methods=['POST'])\n",
    "def generate_audio():\n",
    "    \"\"\"Generate audio from text\"\"\"\n",
    "    try:\n",
    "        data = request.json\n",
    "        text = data.get('text', '')\n",
    "        voice = data.get('voice', 'aria')\n",
    "        speed = float(data.get('speed', 1.0))\n",
    "        \n",
    "        if not text:\n",
    "            return jsonify({'error': 'No text provided'}), 400\n",
    "        \n",
    "        final_audio, sample_rate = generate_audio_parallel(text, voice, speed)\n",
    "        \n",
    "        audio_path = output_dir / f\"audio_{hash(text)}.wav\"\n",
    "        sf.write(str(audio_path), final_audio, sample_rate)\n",
    "        \n",
    "        return send_file(audio_path, mimetype='audio/wav', as_attachment=True)\n",
    "    except Exception as e:\n",
    "        return jsonify({'error': str(e)}), 500\n",
    "\n",
    "@app.route('/generate_image', methods=['POST'])\n",
    "def generate_image():\n",
    "    \"\"\"Generate single image\"\"\"\n",
    "    try:\n",
    "        data = request.json\n",
    "        prompt = data.get('prompt', '')\n",
    "        style = data.get('style', 'cinematic')\n",
    "        \n",
    "        if not prompt:\n",
    "            return jsonify({'error': 'No prompt provided'}), 400\n",
    "        \n",
    "        image = generate_single_image(prompt, style)\n",
    "        \n",
    "        image_path = output_dir / f\"image_{hash(prompt)}.png\"\n",
    "        image.save(image_path, format='PNG')\n",
    "        \n",
    "        return send_file(image_path, mimetype='image/png', as_attachment=True)\n",
    "    except Exception as e:\n",
    "        return jsonify({'error': str(e)}), 500\n",
    "\n",
    "@app.route('/generate_images_batch', methods=['POST'])\n",
    "def generate_images_batch():\n",
    "    \"\"\"Generate multiple images in batch\"\"\"\n",
    "    try:\n",
    "        data = request.json\n",
    "        scenes = data.get('scenes', [])\n",
    "        style = data.get('style', 'cinematic')\n",
    "        \n",
    "        if not scenes:\n",
    "            return jsonify({'error': 'No scenes provided'}), 400\n",
    "        \n",
    "        results = []\n",
    "        for i, scene in enumerate(scenes):\n",
    "            prompt = scene.get('description', '')\n",
    "            if not prompt:\n",
    "                results.append({'success': False, 'error': 'No prompt', 'scene_index': i})\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                image = generate_single_image(prompt, style)\n",
    "                \n",
    "                buffer = io.BytesIO()\n",
    "                image.save(buffer, format='PNG')\n",
    "                image_bytes = buffer.getvalue()\n",
    "                image_base64 = base64.b64encode(image_bytes).decode('utf-8')\n",
    "                \n",
    "                results.append({\n",
    "                    'success': True,\n",
    "                    'image_data': image_base64,\n",
    "                    'scene_index': i,\n",
    "                    'resolution': '1536x864',\n",
    "                    'model': 'DreamShaper-XL',\n",
    "                    'style': style\n",
    "                })\n",
    "            except Exception as e:\n",
    "                results.append({'success': False, 'error': str(e), 'scene_index': i})\n",
    "        \n",
    "        return jsonify({'results': results})\n",
    "    except Exception as e:\n",
    "        return jsonify({'error': str(e)}), 500\n",
    "\n",
    "@app.route('/compile_video', methods=['POST'])\n",
    "def compile_video():\n",
    "    \"\"\"Compile video from media and audio\"\"\"\n",
    "    try:\n",
    "        data = request.json\n",
    "        media_data = data.get('media', [])\n",
    "        media_types = data.get('media_types', [])\n",
    "        audio_data = data.get('audio', '')\n",
    "        durations = data.get('durations', [])\n",
    "        effects = data.get('effects', {})\n",
    "        captions = data.get('captions')\n",
    "        \n",
    "        if not media_data or not audio_data:\n",
    "            return jsonify({'error': 'Media and audio required'}), 400\n",
    "        \n",
    "        if not media_types:\n",
    "            media_types = ['image'] * len(media_data)\n",
    "        \n",
    "        video_path = compile_video_mixed_media(media_data, media_types, audio_data, durations, effects, captions)\n",
    "        \n",
    "        return send_file(video_path, mimetype='video/mp4', as_attachment=True, download_name='final_video.mp4')\n",
    "    except Exception as e:\n",
    "        return jsonify({'error': str(e)}), 500\n",
    "\n",
    "@app.route('/generate_complete_video', methods=['POST'])\n",
    "def generate_complete_video():\n",
    "    \"\"\"Generate complete video with images, voice, and effects\"\"\"\n",
    "    try:\n",
    "        data = request.json\n",
    "        script = data.get('script', '')\n",
    "        image_prompts = data.get('image_prompts', [])\n",
    "        voice_id = data.get('voice_id', 'aria')\n",
    "        effects = data.get('effects', {})\n",
    "        captions = data.get('captions', [])\n",
    "        durations = data.get('durations', [])\n",
    "        style = data.get('style', 'cinematic')\n",
    "        speed = float(data.get('speed', 1.0))\n",
    "        \n",
    "        if not script or not image_prompts:\n",
    "            return jsonify({'error': 'Script and prompts required'}), 400\n",
    "        \n",
    "        print(f\"\\nğŸ“ Generating complete video:\")\n",
    "        print(f\"   Script: {len(script)} chars\")\n",
    "        print(f\"   Images: {len(image_prompts)} scenes\")\n",
    "        print(f\"   Voice: {voice_id}, Style: {style}\")\n",
    "        \n",
    "        # Generate images\n",
    "        print(f\"\\nğŸ¨ Generating {len(image_prompts)} images...\")\n",
    "        image_paths = []\n",
    "        for i, prompt in enumerate(image_prompts, 1):\n",
    "            print(f\"   [{i}/{len(image_prompts)}] {prompt[:50]}...\")\n",
    "            image = generate_single_image(prompt, style)\n",
    "            image_path = output_dir / f\"scene_{i:03d}.png\"\n",
    "            image.save(image_path, format='PNG')\n",
    "            image_paths.append(image_path)\n",
    "        \n",
    "        # Generate voice\n",
    "        print(f\"\\nğŸ¤ Generating voice narration...\")\n",
    "        final_audio, sample_rate = generate_audio_parallel(script, voice_id, speed)\n",
    "        audio_path = output_dir / \"narration.wav\"\n",
    "        sf.write(str(audio_path), final_audio, sample_rate)\n",
    "        audio_duration = len(final_audio) / sample_rate\n",
    "        \n",
    "        # Prepare media\n",
    "        media_data = []\n",
    "        media_types = []\n",
    "        for img_path in image_paths:\n",
    "            with open(img_path, 'rb') as f:\n",
    "                img_bytes = f.read()\n",
    "                img_base64 = base64.b64encode(img_bytes).decode('utf-8')\n",
    "                media_data.append(img_base64)\n",
    "                media_types.append('image')\n",
    "        \n",
    "        with open(audio_path, 'rb') as f:\n",
    "            audio_bytes = f.read()\n",
    "            audio_base64 = base64.b64encode(audio_bytes).decode('utf-8')\n",
    "        \n",
    "        if not durations or len(durations) != len(image_prompts):\n",
    "            duration_per_image = audio_duration / len(image_prompts)\n",
    "            durations = [duration_per_image] * len(image_prompts)\n",
    "        \n",
    "        # Compile video\n",
    "        video_path = compile_video_mixed_media(media_data, media_types, audio_base64, durations, effects, captions if captions else None)\n",
    "        \n",
    "        # Cleanup\n",
    "        for img_path in image_paths:\n",
    "            if img_path.exists():\n",
    "                img_path.unlink()\n",
    "        if audio_path.exists():\n",
    "            audio_path.unlink()\n",
    "        \n",
    "        print(f\"\\nâœ… Complete video generated!\")\n",
    "        \n",
    "        return send_file(video_path, mimetype='video/mp4', as_attachment=True, download_name='final_video.mp4')\n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return jsonify({'error': str(e)}), 500\n",
    "\n",
    "print(\"âœ… Flask server configured with all routes!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# ğŸŒ CELL 7: START SERVER WITH NGROK\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "# Configure ngrok\n",
    "NGROK_AUTH_TOKEN = \"2gacBVJPhjmZXiON9TYHmhI8TkN_39DL8HL8ug3xR7i6QWw9h\"\n",
    "ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
    "\n",
    "# Start Flask server in background\n",
    "def run_server():\n",
    "    app.run(host='0.0.0.0', port=5001, debug=False, use_reloader=False)\n",
    "\n",
    "server_thread = Thread(target=run_server, daemon=True)\n",
    "server_thread.start()\n",
    "\n",
    "time.sleep(3)\n",
    "\n",
    "# Start ngrok tunnel\n",
    "public_url = ngrok.connect(5001, bind_tls=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ‰ GPU SERVER RUNNING - COQUI TTS + DREAMSHAPER XL!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nğŸ“¡ Public URL: {public_url.public_url}\")\n",
    "print(f\"ğŸ–¥ï¸  Local URL:  http://localhost:5001\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"\\nğŸ”¥ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"ğŸ’¾ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "\n",
    "print(\"\\nğŸ“Œ API Endpoints:\")\n",
    "print(f\"   {public_url.public_url}/health\")\n",
    "print(f\"   {public_url.public_url}/generate_audio\")\n",
    "print(f\"   {public_url.public_url}/generate_image\")\n",
    "print(f\"   {public_url.public_url}/generate_images_batch\")\n",
    "print(f\"   {public_url.public_url}/compile_video\")\n",
    "print(f\"   {public_url.public_url}/generate_complete_video\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ”§ UPDATE YOUR BACKEND CONFIG:\")\n",
    "print(\"=\"*80)\n",
    "print(f\"   File: story-video-generator/config/__init__.py\")\n",
    "print(f\"   Set: COLAB_SERVER_URL = '{public_url.public_url}'\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"âœ… ALL FEATURES READY:\")\n",
    "print(\"=\"*80)\n",
    "print(\"   â€¢ Coqui TTS (13 voices, PyTorch GPU)\")\n",
    "print(\"   â€¢ DreamShaper XL (12 styles: cinematic, anime, horror, etc.)\")\n",
    "print(\"   â€¢ FFmpeg video compilation with effects\")\n",
    "print(\"   â€¢ Parallel audio processing (4x faster)\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nğŸŒŸ Server ready! Copy the URL to your config file.\")\n",
    "print(\"\\nPress Ctrl+C to stop.\\n\")\n",
    "\n",
    "# Keep server running\n",
    "try:\n",
    "    while True:\n",
    "        time.sleep(1)\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nğŸ›‘ Server stopped!\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
