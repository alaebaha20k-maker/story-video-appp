{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# ğŸš€ Story Video Generator - GPU Server (Kokoro TTS + SDXL-Turbo + FFmpeg)\n\n**Requirements**: \n- Runtime: GPU (T4, V100, A100)\n- GPU RAM: 15+ GB\n\n**Features**:\n- ğŸ¤ Kokoro TTS (8 voices)\n- ğŸ¨ SDXL-Turbo (AI image generation)\n- ğŸ¬ FFmpeg (video processing)\n- âš¡ On-demand model loading (memory optimized)\n- ğŸŒ Ngrok public URL\n- ğŸ“¡ 3 endpoints: /generate_audio, /generate_image, /generate_images_batch\n\n---\n\n## âš ï¸ IMPORTANT: Enable GPU Runtime\n\n**Before running, make sure GPU is enabled:**\n1. Click: `Runtime` â†’ `Change runtime type`\n2. Select: `Hardware accelerator` â†’ `GPU` â†’ `T4 GPU`\n3. Click: `Save`\n4. Run all cells\n\n---"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# ğŸ“¦ STEP 1: INSTALL DEPENDENCIES\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\nprint(\"ğŸ“¦ Installing dependencies...\\n\")\n\n# âœ… FFmpeg (for video processing)\nprint(\"ğŸ¬ Installing FFmpeg...\")\n!apt-get update -qq\n!apt-get install -y -qq ffmpeg\n!ffmpeg -version | head -n 1\nprint(\"   âœ… FFmpeg installed!\\n\")\n\n# Core dependencies\n!pip install -q flask flask-cors pyngrok\n\n# Kokoro TTS - âœ… FIXED: Install from GitHub source (latest working version)\n!pip install -q git+https://github.com/thewh1teagle/kokoro-onnx.git\n!pip install -q soundfile numpy scipy onnxruntime\n\n# SDXL-Turbo (Diffusers)\n!pip install -q diffusers transformers accelerate safetensors\n\n# Torch (GPU support)\n!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n\nprint(\"\\nâœ… All dependencies installed!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# ğŸ”§ STEP 2: SETUP GPU & IMPORTS\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\n# âœ… Safety check: Ensure Cell 1 dependencies are installed\ntry:\n    from flask_cors import CORS\nexcept ImportError:\n    print(\"âš ï¸  Dependencies not found! Running Cell 1 installations first...\\n\")\n    # Install core dependencies if missing\n    import sys\n    !{sys.executable} -m pip install -q flask flask-cors pyngrok\n    print(\"âœ… Core dependencies installed!\\n\")\n\n# Now import everything\nimport os\nimport gc\nimport torch\nimport json\nfrom flask import Flask, request, jsonify, send_file\nfrom flask_cors import CORS\nfrom pyngrok import ngrok\nfrom pathlib import Path\nfrom threading import Thread\n\n# GPU Detection\nif torch.cuda.is_available():\n    device = 'cuda'\n    gpu_name = torch.cuda.get_device_name(0)\n    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n    print(f\"âœ… GPU ENABLED: {gpu_name}\")\n    print(f\"ğŸ’¾ GPU Memory: {gpu_memory:.1f} GB\")\n    print(f\"ğŸ”¥ CUDA Version: {torch.version.cuda}\")\nelse:\n    device = 'cpu'\n    print(\"âš ï¸  WARNING: GPU NOT DETECTED - Running on CPU (SLOW!)\")\n    print(\"âš ï¸  Please enable GPU: Runtime â†’ Change runtime type â†’ GPU\")\n\nprint(f\"\\nğŸš€ Device: {device}\")\n\n# Create output directory\noutput_dir = Path('/content/outputs')\noutput_dir.mkdir(exist_ok=True)\n\nprint(f\"ğŸ“ Output directory: {output_dir}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# ğŸ§  STEP 3: MEMORY MANAGEMENT (On-Demand Model Loading)\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\n# Global model holders (loaded on-demand)\ntts_pipeline = None\nimg_pipeline = None\n\ndef clear_gpu_memory():\n    \"\"\"Clear GPU memory to make room for other models\"\"\"\n    global tts_pipeline, img_pipeline\n    \n    if tts_pipeline is not None:\n        del tts_pipeline\n        tts_pipeline = None\n        print(\"   ğŸ—‘ï¸  Unloaded TTS model\")\n    \n    if img_pipeline is not None:\n        del img_pipeline\n        img_pipeline = None\n        print(\"   ğŸ—‘ï¸  Unloaded Image model\")\n    \n    gc.collect()\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n        torch.cuda.ipc_collect()\n\ndef load_tts_model():\n    \"\"\"Load Kokoro TTS model (unload image model first)\"\"\"\n    global tts_pipeline, img_pipeline\n    \n    if tts_pipeline is not None:\n        return tts_pipeline\n    \n    print(\"\\nğŸ¤ Loading Kokoro TTS...\")\n    \n    # Unload image model if loaded\n    if img_pipeline is not None:\n        print(\"   âš ï¸  Unloading image model to free memory...\")\n        clear_gpu_memory()\n    \n    # âœ… FIXED: Direct import (kokoro already installed in Cell 1)\n    # No try/except - if this fails, Cell 1 needs to be re-run\n    from kokoro import KPipeline\n    \n    # Initialize with American English (code 'a')\n    tts_pipeline = KPipeline(lang_code='a')\n    if device == 'cuda' and torch.cuda.is_available():\n        tts_pipeline = tts_pipeline.to('cuda')\n        print(\"   âœ… Kokoro TTS loaded on GPU!\")\n    else:\n        print(\"   âœ… Kokoro TTS loaded on CPU!\")\n    \n    return tts_pipeline\n\ndef load_image_model():\n    \"\"\"Load SDXL-Turbo model (unload TTS model first)\"\"\"\n    global tts_pipeline, img_pipeline\n    \n    if img_pipeline is not None:\n        return img_pipeline\n    \n    print(\"\\nğŸ¨ Loading SDXL-Turbo...\")\n    \n    # Unload TTS model if loaded\n    if tts_pipeline is not None:\n        print(\"   âš ï¸  Unloading TTS model to free memory...\")\n        clear_gpu_memory()\n    \n    # âœ… FIXED: Use DiffusionPipeline (works with all diffusers versions)\n    from diffusers import DiffusionPipeline\n    import torch\n    \n    # Load model with correct dtype based on device\n    img_pipeline = DiffusionPipeline.from_pretrained(\n        \"stabilityai/sdxl-turbo\",\n        torch_dtype=torch.float16 if device == 'cuda' else torch.float32,\n        variant=\"fp16\" if device == 'cuda' else None,\n        use_safetensors=True\n    )\n    \n    # Move to GPU and optimize\n    if device == 'cuda':\n        img_pipeline = img_pipeline.to(device)\n        # Memory optimization for T4 GPU (15GB)\n        img_pipeline.enable_attention_slicing()\n        img_pipeline.enable_vae_slicing()\n        print(\"   âš¡ Memory optimization enabled\")\n    \n    print(\"   âœ… SDXL-Turbo loaded!\")\n    return img_pipeline\n\nprint(\"âœ… Memory management functions ready!\")\nprint(\"   ğŸ“Œ Models load on-demand to save memory\")\nprint(\"   ğŸ“Œ Automatic unloading when switching models\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# ğŸ¤ STEP 4: VOICE MAPPING (Kokoro TTS)\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\n# Voice mapping: Backend voice names â†’ Kokoro voice codes\nVOICE_MAPPING = {\n    # Male voices (backend â†’ Kokoro)\n    'guy': 'af_adam',\n    'adam_narration': 'af_adam',\n    'adam_business': 'af_adam',\n    'michael': 'af_michael',\n    'george_gb': 'af_michael',\n    'brian': 'af_adam',\n    'andrew': 'af_adam',\n    'christopher': 'af_michael',\n    'george': 'af_michael',\n    'joey': 'af_adam',\n    \n    # Female voices (backend â†’ Kokoro)\n    'aria': 'af_bella',\n    'sarah_pro': 'af_sarah',\n    'sarah_natural': 'af_sarah',\n    'nicole': 'af_nicole',\n    'emma_gb': 'af_bella',\n    'jenny': 'af_nicole',\n    'sara': 'af_sarah',\n    'jane': 'af_nicole',\n    'libby': 'af_bella',\n    'emma': 'af_bella',\n    'ivy': 'af_bella'\n}\n\nprint(\"âœ… Voice mapping configured:\")\nprint(\"   Backend voices â†’ Kokoro voices\")\nfor key, value in list(VOICE_MAPPING.items())[:8]:\n    print(f\"   {key:20} â†’ {value}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# ğŸŒ STEP 5: FLASK API SERVER\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\napp = Flask(__name__)\nCORS(app)\n\n@app.route('/health', methods=['GET'])\ndef health():\n    \"\"\"Health check endpoint\"\"\"\n    return jsonify({\n        'status': 'healthy',\n        'device': device,\n        'gpu': torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None',\n        'models_loaded': {\n            'tts': tts_pipeline is not None,\n            'image': img_pipeline is not None\n        }\n    })\n\n@app.route('/generate_audio', methods=['POST'])\ndef generate_audio():\n    \"\"\"Generate audio using Kokoro TTS\"\"\"\n    try:\n        data = request.json\n        text = data.get('text', '')\n        voice = data.get('voice', 'guy')\n        speed = float(data.get('speed', 1.0))\n        \n        if not text:\n            return jsonify({'error': 'No text provided'}), 400\n        \n        # Map voice to Kokoro voice code\n        kokoro_voice = VOICE_MAPPING.get(voice, 'af_adam')\n        \n        print(f\"\\nğŸ¤ Generating audio: {voice} â†’ {kokoro_voice}\")\n        print(f\"   Text: {text[:50]}...\")\n        \n        # Load TTS model\n        pipeline = load_tts_model()\n        \n        # Generate audio - âœ… FIXED: Use correct KPipeline API\n        audio_path = output_dir / f\"audio_{hash(text)}.wav\"\n        \n        # Generate audio with KPipeline (returns generator)\n        import soundfile as sf\n        import numpy as np\n        \n        audio_segments = []\n        for i, (gs, ps, audio) in enumerate(pipeline(text, voice=kokoro_voice, speed=speed)):\n            audio_segments.append(audio)\n        \n        # Concatenate all segments\n        if len(audio_segments) > 0:\n            full_audio = np.concatenate(audio_segments)\n            sample_rate = 24000  # Kokoro's sample rate\n            \n            # Save audio to file\n            sf.write(str(audio_path), full_audio, sample_rate)\n        else:\n            raise RuntimeError(\"No audio generated\")\n        \n        print(f\"   âœ… Audio generated: {audio_path.name}\")\n        \n        return send_file(\n            audio_path,\n            mimetype='audio/wav',\n            as_attachment=True,\n            download_name=audio_path.name\n        )\n    \n    except Exception as e:\n        print(f\"   âŒ Error: {e}\")\n        import traceback\n        traceback.print_exc()\n        return jsonify({'error': str(e)}), 500\n\n@app.route('/generate_image', methods=['POST'])\ndef generate_image():\n    \"\"\"Generate a single image using SDXL-Turbo\"\"\"\n    try:\n        data = request.json\n        prompt = data.get('prompt', '')\n        style = data.get('style', 'cinematic')\n        \n        if not prompt:\n            return jsonify({'error': 'No prompt provided'}), 400\n        \n        # Add style to prompt\n        full_prompt = f\"{prompt}, {style} style, high quality, detailed\"\n        \n        print(f\"\\nğŸ¨ Generating image (16:9)...\")\n        print(f\"   Prompt: {full_prompt[:70]}...\")\n        \n        # Load image model\n        pipeline = load_image_model()\n        \n        # âœ… FIXED: Clear cache before generation\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n        \n        # âœ… FIXED: Generate 16:9 ratio (1920x1080) for TikTok/YouTube\n        with torch.inference_mode():\n            image = pipeline(\n                prompt=full_prompt,\n                num_inference_steps=4,  # SDXL-Turbo optimized for 1-4 steps\n                guidance_scale=0.0,      # SDXL-Turbo doesn't use guidance\n                height=1080,             # âœ… 16:9 aspect ratio (Full HD)\n                width=1920\n            ).images[0]\n        \n        # Save image\n        image_path = output_dir / f\"image_{hash(prompt)}.png\"\n        image.save(image_path, format='PNG', optimize=True)\n        \n        print(f\"   âœ… Image generated (1920x1080): {image_path.name}\")\n        \n        # Clear GPU cache\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n        \n        return send_file(\n            image_path,\n            mimetype='image/png',\n            as_attachment=True,\n            download_name=image_path.name\n        )\n    \n    except Exception as e:\n        print(f\"   âŒ Error: {e}\")\n        import traceback\n        traceback.print_exc()\n        return jsonify({'error': str(e)}), 500\n\n@app.route('/generate_images_batch', methods=['POST'])\ndef generate_images_batch():\n    \"\"\"âœ… Generate multiple images in 16:9 ratio and return as base64\"\"\"\n    try:\n        data = request.json\n        scenes = data.get('scenes', [])\n        style = data.get('style', 'cinematic')\n        \n        if not scenes:\n            return jsonify({'error': 'No scenes provided'}), 400\n        \n        print(f\"\\nğŸ¨ Generating {len(scenes)} images (16:9 ratio)...\")\n        \n        # Load image model once\n        pipeline = load_image_model()\n        \n        results = []\n        \n        for i, scene in enumerate(scenes, 1):\n            prompt = scene.get('description', '')\n            if not prompt:\n                results.append({\n                    'success': False,\n                    'error': 'No prompt',\n                    'scene_index': i - 1\n                })\n                continue\n            \n            full_prompt = f\"{prompt}, {style} style, high quality, detailed\"\n            \n            print(f\"   [{i}/{len(scenes)}] {prompt[:50]}...\")\n            \n            try:\n                # âœ… Clear cache BEFORE each generation to prevent OOM\n                if torch.cuda.is_available():\n                    torch.cuda.empty_cache()\n                \n                # âœ… Generate 16:9 ratio (1920x1080) images\n                with torch.inference_mode():\n                    image = pipeline(\n                        prompt=full_prompt,\n                        num_inference_steps=4,\n                        guidance_scale=0.0,\n                        height=1080,  # âœ… 16:9 aspect ratio (Full HD)\n                        width=1920\n                    ).images[0]\n                \n                # âœ… Convert image to base64 for transfer\n                import io\n                import base64\n                \n                buffer = io.BytesIO()\n                image.save(buffer, format='PNG')\n                image_bytes = buffer.getvalue()\n                image_base64 = base64.b64encode(image_bytes).decode('utf-8')\n                \n                results.append({\n                    'success': True,\n                    'image_data': image_base64,  # âœ… Base64 encoded PNG\n                    'scene_index': i - 1,\n                    'size_bytes': len(image_bytes),\n                    'resolution': '1920x1080'  # âœ… 16:9 ratio\n                })\n                \n                print(f\"      âœ… Generated: 1920x1080, {len(image_bytes):,} bytes\")\n            \n            except torch.cuda.OutOfMemoryError as e:\n                print(f\"      âŒ CUDA OOM Error: {e}\")\n                if torch.cuda.is_available():\n                    torch.cuda.empty_cache()\n                results.append({\n                    'success': False,\n                    'error': 'CUDA out of memory',\n                    'scene_index': i - 1\n                })\n            \n            except Exception as e:\n                print(f\"      âŒ Error: {e}\")\n                import traceback\n                traceback.print_exc()\n                results.append({\n                    'success': False,\n                    'error': str(e),\n                    'scene_index': i - 1\n                })\n        \n        success_count = len([r for r in results if r.get('success')])\n        total_size = sum(r.get('size_bytes', 0) for r in results if r.get('success'))\n        \n        print(f\"   âœ… Batch complete: {success_count}/{len(scenes)} successful\")\n        print(f\"   ğŸ“¦ Total data: {total_size:,} bytes ({total_size/1024/1024:.1f} MB)\")\n        print(f\"   ğŸ“ Resolution: 1920x1080 (16:9 aspect ratio)\")\n        \n        return jsonify({'results': results})\n    \n    except Exception as e:\n        print(f\"   âŒ Error: {e}\")\n        import traceback\n        traceback.print_exc()\n        return jsonify({'error': str(e)}), 500\n\nprint(\"\\nâœ… Flask API configured with 4 endpoints:\")\nprint(\"   /health               - Health check\")\nprint(\"   /generate_audio       - Kokoro TTS (single)\")\nprint(\"   /generate_image       - SDXL-Turbo (single, 16:9)\")\nprint(\"   /generate_images_batch - SDXL-Turbo (batch, 16:9, base64)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# ğŸŒ STEP 6: NGROK SETUP (Public URL)\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\nprint(\"\\nğŸ”‘ Setting up Ngrok...\")\n\n# Your Ngrok auth token (already configured!)\nNGROK_AUTH_TOKEN = \"35HuufK0IT26RER84mcvIbRjrog_7grjZvuDXtRPYL5hWLNCK\"\n\nngrok.set_auth_token(NGROK_AUTH_TOKEN)\nprint(\"âœ… Ngrok token configured!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# ğŸš€ STEP 7: START SERVER\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\ndef run_server():\n    \"\"\"Run Flask server in background thread\"\"\"\n    # âœ… FIXED: Use port 5001 to avoid conflict with local backend\n    app.run(host='0.0.0.0', port=5001, debug=False, use_reloader=False)\n\n# Start Flask in background\nprint(\"\\nğŸš€ Starting Flask server...\")\nserver_thread = Thread(target=run_server, daemon=True)\nserver_thread.start()\n\n# Wait for server to start\nimport time\ntime.sleep(3)\n\n# Start Ngrok tunnel - âœ… FIXED: Point to port 5001\nprint(\"ğŸŒ Starting Ngrok tunnel...\")\npublic_url = ngrok.connect(5001, bind_tls=True)\n\nprint(\"\\n\" + \"â•\" * 80)\nprint(\"ğŸ‰ SERVER RUNNING!\")\nprint(\"â•\" * 80)\nprint(f\"\\nğŸ“¡ Public URL: {public_url.public_url}\")\nprint(f\"ğŸ–¥ï¸  Local URL:  http://localhost:5001\")\nprint(f\"\\nğŸ® Device: {device.upper()}\")\nif torch.cuda.is_available():\n    print(f\"ğŸ”¥ GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"ğŸ’¾ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n\nprint(\"\\nğŸ“Œ API Endpoints:\")\nprint(f\"   {public_url.public_url}/health\")\nprint(f\"   {public_url.public_url}/generate_audio\")\nprint(f\"   {public_url.public_url}/generate_image\")\nprint(f\"   {public_url.public_url}/generate_images_batch\")\n\nprint(\"\\n\" + \"â•\" * 80)\nprint(\"ğŸ”§ UPDATE YOUR BACKEND CONFIG:\")\nprint(\"â•\" * 80)\nprint(f\"\\n   File: story-video-generator/config/__init__.py\")\nprint(f\"   Line 16: COLAB_SERVER_URL = '{public_url.public_url}'\")\nprint(\"\\n\" + \"â•\" * 80)\n\nprint(\"\\nğŸ’¡ Server will run until you stop this cell or disconnect from Colab\")\nprint(\"â•\" * 80)\n\n# Keep server running\ntry:\n    while True:\n        time.sleep(1)\nexcept KeyboardInterrupt:\n    print(\"\\nğŸ›‘ Server stopped!\")"
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}