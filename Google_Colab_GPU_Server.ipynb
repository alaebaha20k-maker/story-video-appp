{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \ud83d\ude80 Story Video Generator - GPU Server (Kokoro TTS + SDXL-Turbo)\n",
    "\n",
    "**Requirements**: \n",
    "- Runtime: GPU (T4, V100, A100)\n",
    "- GPU RAM: 15+ GB\n",
    "\n",
    "**Features**:\n",
    "- \ud83c\udfa4 Kokoro TTS (8 voices)\n",
    "- \ud83c\udfa8 SDXL-Turbo (AI image generation)\n",
    "- \u26a1 On-demand model loading (memory optimized)\n",
    "- \ud83c\udf10 Ngrok public URL\n",
    "- \ud83d\udce1 3 endpoints: /generate_audio, /generate_image, /generate_images_batch\n",
    "\n",
    "---\n",
    "\n",
    "## \u26a0\ufe0f IMPORTANT: Enable GPU Runtime\n",
    "\n",
    "**Before running, make sure GPU is enabled:**\n",
    "1. Click: `Runtime` \u2192 `Change runtime type`\n",
    "2. Select: `Hardware accelerator` \u2192 `GPU` \u2192 `T4 GPU`\n",
    "3. Click: `Save`\n",
    "4. Run all cells\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n# \ud83d\udce6 STEP 1: INSTALL DEPENDENCIES\n# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n\nprint(\"\ud83d\udce6 Installing dependencies...\\n\")\n\n# Core dependencies\n!pip install -q flask flask-cors pyngrok\n\n# Kokoro TTS (ONNX version)\n!pip install -q kokoro-onnx soundfile\n\n# SDXL-Turbo (Diffusers)\n!pip install -q diffusers transformers accelerate\n\n# Torch (GPU support)\n!pip install -q torch torchvision --index-url https://download.pytorch.org/whl/cu121\n\nprint(\"\\n\u2705 Dependencies installed!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "# \ud83d\udd27 STEP 2: SETUP GPU & IMPORTS\n",
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import torch\n",
    "import json\n",
    "from flask import Flask, request, jsonify, send_file\n",
    "from flask_cors import CORS\n",
    "from pyngrok import ngrok\n",
    "from pathlib import Path\n",
    "from threading import Thread\n",
    "\n",
    "# GPU Detection\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "    print(f\"\u2705 GPU ENABLED: {gpu_name}\")\n",
    "    print(f\"\ud83d\udcbe GPU Memory: {gpu_memory:.1f} GB\")\n",
    "    print(f\"\ud83d\udd25 CUDA Version: {torch.version.cuda}\")\n",
    "else:\n",
    "    device = 'cpu'\n",
    "    print(\"\u26a0\ufe0f  WARNING: GPU NOT DETECTED - Running on CPU (SLOW!)\")\n",
    "    print(\"\u26a0\ufe0f  Please enable GPU: Runtime \u2192 Change runtime type \u2192 GPU\")\n",
    "\n",
    "print(f\"\\n\ud83d\ude80 Device: {device}\")\n",
    "\n",
    "# Create output directory\n",
    "output_dir = Path('/content/outputs')\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"\ud83d\udcc1 Output directory: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n# \ud83e\udde0 STEP 3: MEMORY MANAGEMENT (On-Demand Model Loading)\n# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n\n# Global model holders (loaded on-demand)\ntts_pipeline = None\nimg_pipeline = None\n\ndef clear_gpu_memory():\n    \"\"\"Clear GPU memory to make room for other models\"\"\"\n    global tts_pipeline, img_pipeline\n    \n    if tts_pipeline is not None:\n        del tts_pipeline\n        tts_pipeline = None\n        print(\"   \ud83d\uddd1\ufe0f  Unloaded TTS model\")\n    \n    if img_pipeline is not None:\n        del img_pipeline\n        img_pipeline = None\n        print(\"   \ud83d\uddd1\ufe0f  Unloaded Image model\")\n    \n    gc.collect()\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n        torch.cuda.ipc_collect()\n\ndef load_tts_model():\n    \"\"\"Load Kokoro TTS model (unload image model first)\"\"\"\n    global tts_pipeline, img_pipeline\n    \n    if tts_pipeline is not None:\n        return tts_pipeline\n    \n    print(\"\\n\ud83c\udfa4 Loading Kokoro TTS...\")\n    \n    # Unload image model if loaded\n    if img_pipeline is not None:\n        print(\"   \u26a0\ufe0f  Unloading image model to free memory...\")\n        clear_gpu_memory()\n    \n    # \u2705 FIXED: Correct import for kokoro-onnx package\n    from kokoro_onnx import Kokoro\n    tts_pipeline = Kokoro()\n    \n    print(\"   \u2705 Kokoro TTS loaded!\")\n    return tts_pipeline\n\ndef load_image_model():\n    \"\"\"Load SDXL-Turbo model (unload TTS model first)\"\"\"\n    global tts_pipeline, img_pipeline\n    \n    if img_pipeline is not None:\n        return img_pipeline\n    \n    print(\"\\n\ud83c\udfa8 Loading SDXL-Turbo...\")\n    \n    # Unload TTS model if loaded\n    if tts_pipeline is not None:\n        print(\"   \u26a0\ufe0f  Unloading TTS model to free memory...\")\n        clear_gpu_memory()\n    \n    # \u2705 FIXED: Correct import for diffusers\n    from diffusers import DiffusionPipeline\n    \n    img_pipeline = DiffusionPipeline.from_pretrained(\n        \"stabilityai/sdxl-turbo\",\n        torch_dtype=torch.float16 if device == 'cuda' else torch.float32,\n        variant=\"fp16\" if device == 'cuda' else None\n    )\n    \n    if device == 'cuda':\n        img_pipeline = img_pipeline.to(device)\n        # Memory optimization\n        img_pipeline.enable_attention_slicing()\n        img_pipeline.enable_vae_slicing()\n        print(\"   \u26a1 Memory optimization enabled (attention + VAE slicing)\")\n    \n    print(\"   \u2705 SDXL-Turbo loaded!\")\n    return img_pipeline\n\nprint(\"\u2705 Memory management functions ready!\")\nprint(\"   \ud83d\udccc Models load on-demand to save memory\")\nprint(\"   \ud83d\udccc Automatic unloading when switching models\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n# \ud83c\udfa4 STEP 4: VOICE MAPPING (Kokoro TTS)\n# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n\n# Voice mapping: Backend voice names \u2192 Kokoro voice codes\nVOICE_MAPPING = {\n    # Male voices (backend \u2192 Kokoro)\n    'guy': 'af_adam',\n    'adam_narration': 'af_adam',\n    'adam_business': 'af_adam',\n    'michael': 'af_michael',\n    'george_gb': 'af_michael',\n    'brian': 'af_adam',\n    'andrew': 'af_adam',\n    'christopher': 'af_michael',\n    'george': 'af_michael',\n    'joey': 'af_adam',\n    \n    # Female voices (backend \u2192 Kokoro)\n    'aria': 'af_bella',\n    'sarah_pro': 'af_sarah',\n    'sarah_natural': 'af_sarah',\n    'nicole': 'af_nicole',\n    'emma_gb': 'af_bella',\n    'jenny': 'af_nicole',\n    'sara': 'af_sarah',\n    'jane': 'af_nicole',\n    'libby': 'af_bella',\n    'emma': 'af_bella',\n    'ivy': 'af_bella'\n}\n\nprint(\"\u2705 Voice mapping configured:\")\nprint(\"   Backend voices \u2192 Kokoro voices\")\nfor key, value in list(VOICE_MAPPING.items())[:8]:\n    print(f\"   {key:20} \u2192 {value}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550",
    "# CELL 5: FLASK API SERVER",
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550",
    "",
    "app = Flask(__name__)",
    "CORS(app)",
    "",
    "# Health Check",
    "@app.route('/health', methods=['GET'])",
    "def health():",
    "    \"\"\"Health check endpoint\"\"\"",
    "    return jsonify({",
    "        'status': 'healthy',",
    "        'device': device,",
    "        'gpu': torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None',",
    "        'models_loaded': {",
    "            'tts': tts_pipeline is not None,",
    "            'image': img_pipeline is not None",
    "        }",
    "    })",
    "",
    "# Endpoint 1: Generate Audio (Kokoro TTS)",
    "@app.route('/generate_audio', methods=['POST'])",
    "def generate_audio():",
    "    \"\"\"Generate audio using Kokoro TTS\"\"\"",
    "    try:",
    "        data = request.json",
    "        text = data.get('text', '')",
    "        voice = data.get('voice', 'guy')",
    "        speed = float(data.get('speed', 1.0))",
    "        ",
    "        if not prompt:",
    "            return jsonify({'error': 'No text provided'}), 400",
    "        ",
    "        # Map voice to Kokoro voice code",
    "        kokoro_voice = VOICE_MAPPING.get(voice, 'af_adam')",
    "        ",
    "        print(f\"\\n\ud83c\udfa4 Generating audio: {voice} \u2192 {kokoro_voice}\")",
    "        print(f\"   Text: {text[:50]}...\")",
    "        ",
    "        # Load TTS model",
    "        pipeline = load_tts_model()",
    "        ",
    "        # Generate audio - \u2705 FIXED: Use correct Kokoro API",
    "        audio_path = output_dir / f\"audio_{hash(text)}.wav\"",
    "        ",
    "        # Generate samples (Kokoro returns audio samples)",
    "        samples = pipeline.create(",
    "            text,",
    "            voice=kokoro_voice,",
    "            speed=speed",
    "        )",
    "        ",
    "        # Save audio to file",
    "        import soundfile as sf",
    "        sf.write(str(audio_path), samples, 24000)  # Kokoro uses 24kHz",
    "        ",
    "        print(f\"   \u2705 Audio generated: {audio_path.name}\")",
    "        ",
    "        return send_file(",
    "            audio_path,",
    "            mimetype='audio/wav',",
    "            as_attachment=True,",
    "            download_name=audio_path.name",
    "        )",
    "    ",
    "    except Exception as e:",
    "        print(f\"   \u274c Error: {e}\")",
    "        import traceback",
    "        traceback.print_exc()",
    "        return jsonify({'error': str(e)}), 500",
    "",
    "# Endpoint 2: Generate Single Image (SDXL-Turbo)",
    "@app.route('/generate_image', methods=['POST'])",
    "def generate_image():",
    "    \"\"\"Generate a single image using SDXL-Turbo\"\"\"",
    "    try:",
    "        data = request.json",
    "        prompt = data.get('prompt', '')",
    "        style = data.get('style', 'cinematic')",
    "        ",
    "        if not text:",
    "            return jsonify({'error': 'No prompt provided'}), 400",
    "        ",
    "        # Add style to prompt",
    "        full_prompt = f\"{prompt}, {style} style, high quality, detailed\"",
    "        ",
    "        print(f\"\\n\ud83c\udfa8 Generating image...\")",
    "        print(f\"   Prompt: {full_prompt[:70]}...\")",
    "        ",
    "        # Load image model",
    "        pipeline = load_image_model()",
    "        ",
    "        # Generate image",
    "        image = pipeline(",
    "            prompt=full_prompt,",
    "            num_inference_steps=4,  # SDXL-Turbo optimized steps",
    "            guidance_scale=0.0,     # SDXL-Turbo doesn't need guidance",
    "            height=1080,",
    "            width=1920",
    "        ).images[0]",
    "        ",
    "        # Save image",
    "        image_path = output_dir / f\"image_{hash(prompt)}.png\"",
    "        image.save(image_path)",
    "        ",
    "        print(f\"   \u2705 Image generated: {image_path.name}\")",
    "        ",
    "        return send_file(",
    "            image_path,",
    "            mimetype='image/png',",
    "            as_attachment=True,",
    "            download_name=image_path.name",
    "        )",
    "    ",
    "    except Exception as e:",
    "        print(f\"   \u274c Error: {e}\")",
    "        import traceback",
    "        traceback.print_exc()",
    "        return jsonify({'error': str(e)}), 500",
    "",
    "# Endpoint 3: Generate Batch Images (SDXL-Turbo)",
    "@app.route('/generate_images_batch', methods=['POST'])",
    "def generate_images_batch():",
    "    \"\"\"Generate multiple images in batch\"\"\"",
    "    try:",
    "        data = request.json",
    "        scenes = data.get('scenes', [])",
    "        style = data.get('style', 'cinematic')",
    "        ",
    "        if not scenes:",
    "            return jsonify({'error': 'No scenes provided'}), 400",
    "        ",
    "        print(f\"\\n\ud83c\udfa8 Generating {len(scenes)} images in batch...\")",
    "        ",
    "        # Load image model once",
    "        pipeline = load_image_model()",
    "        ",
    "        results = []",
    "        ",
    "        for i, scene in enumerate(scenes, 1):",
    "            prompt = scene.get('description', '')",
    "            if not prompt:",
    "                results.append({'error': 'No prompt', 'image_path': None})",
    "                continue",
    "            ",
    "            full_prompt = f\"{prompt}, {style} style, high quality, detailed\"",
    "            ",
    "            print(f\"   [{i}/{len(scenes)}] {prompt[:50]}...\")",
    "            ",
    "            try:",
    "                # Generate image",
    "                image = pipeline(",
    "                    prompt=full_prompt,",
    "                    num_inference_steps=4,",
    "                    guidance_scale=0.0,",
    "                    height=1080,",
    "                    width=1920",
    "                ).images[0]",
    "                ",
    "                # Save image",
    "                image_path = output_dir / f\"batch_{i}_{hash(prompt)}.png\"",
    "                image.save(image_path)",
    "                ",
    "                results.append({",
    "                    'success': True,",
    "                    'image_path': str(image_path),",
    "                    'scene_index': i - 1",
    "                })",
    "                ",
    "                # Clear cache periodically",
    "                if i % 5 == 0 and torch.cuda.is_available():",
    "                    torch.cuda.empty_cache()",
    "            ",
    "            except Exception as e:",
    "                print(f\"      \u274c Error: {e}\")",
    "                results.append({",
    "                    'success': False,",
    "                    'error': str(e),",
    "                    'scene_index': i - 1",
    "                })",
    "        ",
    "        print(f\"   \u2705 Batch complete: {len([r for r in results if r.get('success')])}/{len(scenes)} successful\")",
    "        ",
    "        return jsonify({'results': results})",
    "    ",
    "    except Exception as e:",
    "        print(f\"   \u274c Error: {e}\")",
    "        import traceback",
    "        traceback.print_exc()",
    "        return jsonify({'error': str(e)}), 500",
    "",
    "print(\"\\n\u2705 Flask API configured with 4 endpoints:\")",
    "print(\"   /health               - Health check\")",
    "print(\"   /generate_audio       - Kokoro TTS (single)\")",
    "print(\"   /generate_image       - SDXL-Turbo (single)\")",
    "print(\"   /generate_images_batch - SDXL-Turbo (batch)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n# \ud83c\udf10 STEP 6: NGROK SETUP (Public URL)\n# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n\nprint(\"\\n\ud83d\udd11 Setting up Ngrok...\")\n\n# Your Ngrok auth token (already configured!)\nNGROK_AUTH_TOKEN = \"35HuufK0IT26RER84mcvIbRjrog_7grjZvuDXtRPYL5hWLNCK\"\n\nngrok.set_auth_token(NGROK_AUTH_TOKEN)\nprint(\"\u2705 Ngrok token configured!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "# \ud83d\ude80 STEP 7: START SERVER\n",
    "# \u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\n",
    "\n",
    "def run_server():\n",
    "    \"\"\"Run Flask server in background thread\"\"\"\n",
    "    app.run(port=5000, debug=False, use_reloader=False)\n",
    "\n",
    "# Start Flask in background\n",
    "print(\"\\n\ud83d\ude80 Starting Flask server...\")\n",
    "server_thread = Thread(target=run_server, daemon=True)\n",
    "server_thread.start()\n",
    "\n",
    "# Wait for server to start\n",
    "import time\n",
    "time.sleep(3)\n",
    "\n",
    "# Start Ngrok tunnel\n",
    "print(\"\ud83c\udf10 Starting Ngrok tunnel...\")\n",
    "public_url = ngrok.connect(5000, bind_tls=True)\n",
    "\n",
    "print(\"\\n\" + \"\u2550\" * 80)\n",
    "print(\"\ud83c\udf89 SERVER RUNNING!\")\n",
    "print(\"\u2550\" * 80)\n",
    "print(f\"\\n\ud83d\udce1 Public URL: {public_url.public_url}\")\n",
    "print(f\"\ud83d\udda5\ufe0f  Local URL:  http://localhost:5000\")\n",
    "print(f\"\\n\ud83c\udfae Device: {device.upper()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"\ud83d\udd25 GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"\ud83d\udcbe GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "\n",
    "print(\"\\n\ud83d\udccc API Endpoints:\")\n",
    "print(f\"   {public_url.public_url}/health\")\n",
    "print(f\"   {public_url.public_url}/generate_audio\")\n",
    "print(f\"   {public_url.public_url}/generate_image\")\n",
    "print(f\"   {public_url.public_url}/generate_images_batch\")\n",
    "\n",
    "print(\"\\n\ud83d\udd27 Update your backend config:\")\n",
    "print(f\"   KOKORO_API_URL = '{public_url.public_url}'\")\n",
    "print(f\"   SDXL_API_URL = '{public_url.public_url}'\")\n",
    "\n",
    "print(\"\\n\ud83d\udca1 Server will run until you stop this cell or disconnect from Colab\")\n",
    "print(\"\u2550\" * 80)\n",
    "\n",
    "# Keep server running\n",
    "try:\n",
    "    while True:\n",
    "        time.sleep(1)\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n\ud83d\uded1 Server stopped!\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}