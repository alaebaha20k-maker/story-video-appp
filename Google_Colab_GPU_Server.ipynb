{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸš€ Story Video Generator - GPU Server (Kokoro TTS + SDXL-Turbo)\n",
    "\n",
    "**Requirements**: \n",
    "- Runtime: GPU (T4, V100, A100)\n",
    "- GPU RAM: 15+ GB\n",
    "\n",
    "**Features**:\n",
    "- ğŸ¤ Kokoro TTS (8 voices)\n",
    "- ğŸ¨ SDXL-Turbo (AI image generation)\n",
    "- âš¡ On-demand model loading (memory optimized)\n",
    "- ğŸŒ Ngrok public URL\n",
    "- ğŸ“¡ 3 endpoints: /generate_audio, /generate_image, /generate_images_batch\n",
    "\n",
    "---\n",
    "\n",
    "## âš ï¸ IMPORTANT: Enable GPU Runtime\n",
    "\n",
    "**Before running, make sure GPU is enabled:**\n",
    "1. Click: `Runtime` â†’ `Change runtime type`\n",
    "2. Select: `Hardware accelerator` â†’ `GPU` â†’ `T4 GPU`\n",
    "3. Click: `Save`\n",
    "4. Run all cells\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# ğŸ“¦ STEP 1: INSTALL DEPENDENCIES\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "print(\"ğŸ“¦ Installing dependencies...\\n\")\n",
    "\n",
    "# Core dependencies\n",
    "!pip install -q flask flask-cors pyngrok\n",
    "\n",
    "# Kokoro TTS\n",
    "!pip install -q kokoro-onnx\n",
    "\n",
    "# SDXL-Turbo (Diffusers)\n",
    "!pip install -q diffusers transformers accelerate\n",
    "\n",
    "# Torch (GPU support)\n",
    "!pip install -q torch torchvision --index-url https://download.pytorch.org/whl/cu121\n",
    "\n",
    "print(\"\\nâœ… Dependencies installed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# ğŸ”§ STEP 2: SETUP GPU & IMPORTS\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "import os\n",
    "import gc\n",
    "import torch\n",
    "import json\n",
    "from flask import Flask, request, jsonify, send_file\n",
    "from flask_cors import CORS\n",
    "from pyngrok import ngrok\n",
    "from pathlib import Path\n",
    "from threading import Thread\n",
    "\n",
    "# GPU Detection\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "    print(f\"âœ… GPU ENABLED: {gpu_name}\")\n",
    "    print(f\"ğŸ’¾ GPU Memory: {gpu_memory:.1f} GB\")\n",
    "    print(f\"ğŸ”¥ CUDA Version: {torch.version.cuda}\")\n",
    "else:\n",
    "    device = 'cpu'\n",
    "    print(\"âš ï¸  WARNING: GPU NOT DETECTED - Running on CPU (SLOW!)\")\n",
    "    print(\"âš ï¸  Please enable GPU: Runtime â†’ Change runtime type â†’ GPU\")\n",
    "\n",
    "print(f\"\\nğŸš€ Device: {device}\")\n",
    "\n",
    "# Create output directory\n",
    "output_dir = Path('/content/outputs')\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"ğŸ“ Output directory: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# ğŸ§  STEP 3: MEMORY MANAGEMENT (On-Demand Model Loading)\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "# Global model holders (loaded on-demand)\n",
    "tts_pipeline = None\n",
    "img_pipeline = None\n",
    "\n",
    "def clear_gpu_memory():\n",
    "    \"\"\"Clear GPU memory to make room for other models\"\"\"\n",
    "    global tts_pipeline, img_pipeline\n",
    "    \n",
    "    if tts_pipeline is not None:\n",
    "        del tts_pipeline\n",
    "        tts_pipeline = None\n",
    "        print(\"   ğŸ—‘ï¸  Unloaded TTS model\")\n",
    "    \n",
    "    if img_pipeline is not None:\n",
    "        del img_pipeline\n",
    "        img_pipeline = None\n",
    "        print(\"   ğŸ—‘ï¸  Unloaded Image model\")\n",
    "    \n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.ipc_collect()\n",
    "\n",
    "def load_tts_model():\n",
    "    \"\"\"Load Kokoro TTS model (unload image model first)\"\"\"\n",
    "    global tts_pipeline, img_pipeline\n",
    "    \n",
    "    if tts_pipeline is not None:\n",
    "        return tts_pipeline\n",
    "    \n",
    "    print(\"\\nğŸ¤ Loading Kokoro TTS...\")\n",
    "    \n",
    "    # Unload image model if loaded\n",
    "    if img_pipeline is not None:\n",
    "        print(\"   âš ï¸  Unloading image model to free memory...\")\n",
    "        clear_gpu_memory()\n",
    "    \n",
    "    from kokoro import KPipeline\n",
    "    tts_pipeline = KPipeline(lang_code='a')  # 'a' = American English\n",
    "    \n",
    "    print(\"   âœ… Kokoro TTS loaded!\")\n",
    "    return tts_pipeline\n",
    "\n",
    "def load_image_model():\n",
    "    \"\"\"Load SDXL-Turbo model (unload TTS model first)\"\"\"\n",
    "    global tts_pipeline, img_pipeline\n",
    "    \n",
    "    if img_pipeline is not None:\n",
    "        return img_pipeline\n",
    "    \n",
    "    print(\"\\nğŸ¨ Loading SDXL-Turbo...\")\n",
    "    \n",
    "    # Unload TTS model if loaded\n",
    "    if tts_pipeline is not None:\n",
    "        print(\"   âš ï¸  Unloading TTS model to free memory...\")\n",
    "        clear_gpu_memory()\n",
    "    \n",
    "    from diffusers import AutoPipelineForText2Image\n",
    "    \n",
    "    img_pipeline = AutoPipelineForText2Image.from_pretrained(\n",
    "        \"stabilityai/sdxl-turbo\",\n",
    "        torch_dtype=torch.float16 if device == 'cuda' else torch.float32,\n",
    "        variant=\"fp16\" if device == 'cuda' else None\n",
    "    )\n",
    "    \n",
    "    if device == 'cuda':\n",
    "        img_pipeline = img_pipeline.to(device)\n",
    "        # Memory optimization\n",
    "        img_pipeline.enable_attention_slicing()\n",
    "        img_pipeline.enable_vae_slicing()\n",
    "        print(\"   âš¡ Memory optimization enabled (attention + VAE slicing)\")\n",
    "    \n",
    "    print(\"   âœ… SDXL-Turbo loaded!\")\n",
    "    return img_pipeline\n",
    "\n",
    "print(\"âœ… Memory management functions ready!\")\n",
    "print(\"   ğŸ“Œ Models load on-demand to save memory\")\n",
    "print(\"   ğŸ“Œ Automatic unloading when switching models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# ğŸ¤ STEP 4: VOICE MAPPING (Kokoro TTS)\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\n# Voice mapping: Backend voice names â†’ Kokoro voice codes\nVOICE_MAPPING = {\n    # Male voices (backend â†’ Kokoro)\n    'guy': 'af_adam',\n    'adam_narration': 'af_adam',\n    'adam_business': 'af_adam',\n    'michael': 'af_michael',\n    'george_gb': 'af_michael',\n    'brian': 'af_adam',\n    'andrew': 'af_adam',\n    'christopher': 'af_michael',\n    'george': 'af_michael',\n    'joey': 'af_adam',\n    \n    # Female voices (backend â†’ Kokoro)\n    'aria': 'af_bella',\n    'sarah_pro': 'af_sarah',\n    'sarah_natural': 'af_sarah',\n    'nicole': 'af_nicole',\n    'emma_gb': 'af_bella',\n    'jenny': 'af_nicole',\n    'sara': 'af_sarah',\n    'jane': 'af_nicole',\n    'libby': 'af_bella',\n    'emma': 'af_bella',\n    'ivy': 'af_bella'\n}\n\nprint(\"âœ… Voice mapping configured:\")\nprint(\"   Backend voices â†’ Kokoro voices\")\nfor key, value in list(VOICE_MAPPING.items())[:8]:\n    print(f\"   {key:20} â†’ {value}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# ğŸŒ STEP 5: FLASK API SERVER\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "app = Flask(__name__)\n",
    "CORS(app)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Health Check\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "@app.route('/health', methods=['GET'])\n",
    "def health():\n",
    "    \"\"\"Health check endpoint\"\"\"\n",
    "    return jsonify({\n",
    "        'status': 'healthy',\n",
    "        'device': device,\n",
    "        'gpu': torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None',\n",
    "        'models_loaded': {\n",
    "            'tts': tts_pipeline is not None,\n",
    "            'image': img_pipeline is not None\n",
    "        }\n",
    "    })\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Endpoint 1: Generate Audio (Kokoro TTS)\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "@app.route('/generate_audio', methods=['POST'])\n",
    "def generate_audio():\n",
    "    \"\"\"Generate audio using Kokoro TTS\"\"\"\n",
    "    try:\n",
    "        data = request.json\n",
    "        text = data.get('text', '')\n",
    "        voice = data.get('voice', 'guy')\n",
    "        speed = float(data.get('speed', 1.0))\n",
    "        \n",
    "        if not text:\n",
    "            return jsonify({'error': 'No text provided'}), 400\n",
    "        \n",
    "        # Map voice to Kokoro voice code\n",
    "        kokoro_voice = VOICE_MAPPING.get(voice, 'af_adam')\n",
    "        \n",
    "        print(f\"\\nğŸ¤ Generating audio: {voice} â†’ {kokoro_voice}\")\n",
    "        print(f\"   Text: {text[:50]}...\")\n",
    "        \n",
    "        # Load TTS model\n",
    "        pipeline = load_tts_model()\n",
    "        \n",
    "        # Generate audio\n",
    "        audio_path = output_dir / f\"audio_{hash(text)}.wav\"\n",
    "        \n",
    "        pipeline(\n",
    "            text,\n",
    "            voice=kokoro_voice,\n",
    "            speed=speed,\n",
    "            output=str(audio_path)\n",
    "        )\n",
    "        \n",
    "        print(f\"   âœ… Audio generated: {audio_path.name}\")\n",
    "        \n",
    "        return send_file(\n",
    "            audio_path,\n",
    "            mimetype='audio/wav',\n",
    "            as_attachment=True,\n",
    "            download_name=audio_path.name\n",
    "        )\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ Error: {e}\")\n",
    "        return jsonify({'error': str(e)}), 500\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Endpoint 2: Generate Single Image (SDXL-Turbo)\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "@app.route('/generate_image', methods=['POST'])\n",
    "def generate_image():\n",
    "    \"\"\"Generate a single image using SDXL-Turbo\"\"\"\n",
    "    try:\n",
    "        data = request.json\n",
    "        prompt = data.get('prompt', '')\n",
    "        style = data.get('style', 'cinematic')\n",
    "        \n",
    "        if not prompt:\n",
    "            return jsonify({'error': 'No prompt provided'}), 400\n",
    "        \n",
    "        # Add style to prompt\n",
    "        full_prompt = f\"{prompt}, {style} style, high quality, detailed\"\n",
    "        \n",
    "        print(f\"\\nğŸ¨ Generating image...\")\n",
    "        print(f\"   Prompt: {full_prompt[:70]}...\")\n",
    "        \n",
    "        # Load image model\n",
    "        pipeline = load_image_model()\n",
    "        \n",
    "        # Generate image\n",
    "        image = pipeline(\n",
    "            prompt=full_prompt,\n",
    "            num_inference_steps=4,  # SDXL-Turbo optimized steps\n",
    "            guidance_scale=0.0,     # SDXL-Turbo doesn't need guidance\n",
    "            height=1080,\n",
    "            width=1920\n",
    "        ).images[0]\n",
    "        \n",
    "        # Save image\n",
    "        image_path = output_dir / f\"image_{hash(prompt)}.png\"\n",
    "        image.save(image_path)\n",
    "        \n",
    "        print(f\"   âœ… Image generated: {image_path.name}\")\n",
    "        \n",
    "        return send_file(\n",
    "            image_path,\n",
    "            mimetype='image/png',\n",
    "            as_attachment=True,\n",
    "            download_name=image_path.name\n",
    "        )\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ Error: {e}\")\n",
    "        return jsonify({'error': str(e)}), 500\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Endpoint 3: Generate Batch Images (SDXL-Turbo)\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "@app.route('/generate_images_batch', methods=['POST'])\n",
    "def generate_images_batch():\n",
    "    \"\"\"Generate multiple images in batch\"\"\"\n",
    "    try:\n",
    "        data = request.json\n",
    "        scenes = data.get('scenes', [])\n",
    "        style = data.get('style', 'cinematic')\n",
    "        \n",
    "        if not scenes:\n",
    "            return jsonify({'error': 'No scenes provided'}), 400\n",
    "        \n",
    "        print(f\"\\nğŸ¨ Generating {len(scenes)} images in batch...\")\n",
    "        \n",
    "        # Load image model once\n",
    "        pipeline = load_image_model()\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        for i, scene in enumerate(scenes, 1):\n",
    "            prompt = scene.get('description', '')\n",
    "            if not prompt:\n",
    "                results.append({'error': 'No prompt', 'image_path': None})\n",
    "                continue\n",
    "            \n",
    "            full_prompt = f\"{prompt}, {style} style, high quality, detailed\"\n",
    "            \n",
    "            print(f\"   [{i}/{len(scenes)}] {prompt[:50]}...\")\n",
    "            \n",
    "            try:\n",
    "                # Generate image\n",
    "                image = pipeline(\n",
    "                    prompt=full_prompt,\n",
    "                    num_inference_steps=4,\n",
    "                    guidance_scale=0.0,\n",
    "                    height=1080,\n",
    "                    width=1920\n",
    "                ).images[0]\n",
    "                \n",
    "                # Save image\n",
    "                image_path = output_dir / f\"batch_{i}_{hash(prompt)}.png\"\n",
    "                image.save(image_path)\n",
    "                \n",
    "                results.append({\n",
    "                    'success': True,\n",
    "                    'image_path': str(image_path),\n",
    "                    'scene_index': i - 1\n",
    "                })\n",
    "                \n",
    "                # Clear cache periodically\n",
    "                if i % 5 == 0 and torch.cuda.is_available():\n",
    "                    torch.cuda.empty_cache()\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"      âŒ Error: {e}\")\n",
    "                results.append({\n",
    "                    'success': False,\n",
    "                    'error': str(e),\n",
    "                    'scene_index': i - 1\n",
    "                })\n",
    "        \n",
    "        print(f\"   âœ… Batch complete: {len([r for r in results if r.get('success')])}/{len(scenes)} successful\")\n",
    "        \n",
    "        return jsonify({'results': results})\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"   âŒ Error: {e}\")\n",
    "        return jsonify({'error': str(e)}), 500\n",
    "\n",
    "print(\"\\nâœ… Flask API configured with 3 endpoints:\")\n",
    "print(\"   /health               - Health check\")\n",
    "print(\"   /generate_audio       - Kokoro TTS (single)\")\n",
    "print(\"   /generate_image       - SDXL-Turbo (single)\")\n",
    "print(\"   /generate_images_batch - SDXL-Turbo (batch)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# ğŸŒ STEP 6: NGROK SETUP (Public URL)\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "print(\"\\nğŸ”‘ Setting up Ngrok...\")\n",
    "print(\"   âš ï¸  You need an Ngrok auth token!\")\n",
    "print(\"   âš ï¸  Get free token: https://dashboard.ngrok.com/get-started/your-authtoken\\n\")\n",
    "\n",
    "# Set your Ngrok auth token here\n",
    "NGROK_AUTH_TOKEN = \"YOUR_NGROK_TOKEN_HERE\"  # â† REPLACE WITH YOUR TOKEN\n",
    "\n",
    "if NGROK_AUTH_TOKEN == \"YOUR_NGROK_TOKEN_HERE\":\n",
    "    print(\"âš ï¸  WARNING: Please set your Ngrok auth token above!\")\n",
    "    print(\"   1. Go to: https://dashboard.ngrok.com/get-started/your-authtoken\")\n",
    "    print(\"   2. Copy your token\")\n",
    "    print(\"   3. Replace 'YOUR_NGROK_TOKEN_HERE' with your token\")\n",
    "    print(\"   4. Re-run this cell\\n\")\n",
    "else:\n",
    "    ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
    "    print(\"âœ… Ngrok token configured!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# ğŸš€ STEP 7: START SERVER\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "def run_server():\n",
    "    \"\"\"Run Flask server in background thread\"\"\"\n",
    "    app.run(port=5000, debug=False, use_reloader=False)\n",
    "\n",
    "# Start Flask in background\n",
    "print(\"\\nğŸš€ Starting Flask server...\")\n",
    "server_thread = Thread(target=run_server, daemon=True)\n",
    "server_thread.start()\n",
    "\n",
    "# Wait for server to start\n",
    "import time\n",
    "time.sleep(3)\n",
    "\n",
    "# Start Ngrok tunnel\n",
    "print(\"ğŸŒ Starting Ngrok tunnel...\")\n",
    "public_url = ngrok.connect(5000, bind_tls=True)\n",
    "\n",
    "print(\"\\n\" + \"â•\" * 80)\n",
    "print(\"ğŸ‰ SERVER RUNNING!\")\n",
    "print(\"â•\" * 80)\n",
    "print(f\"\\nğŸ“¡ Public URL: {public_url.public_url}\")\n",
    "print(f\"ğŸ–¥ï¸  Local URL:  http://localhost:5000\")\n",
    "print(f\"\\nğŸ® Device: {device.upper()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"ğŸ”¥ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"ğŸ’¾ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "\n",
    "print(\"\\nğŸ“Œ API Endpoints:\")\n",
    "print(f\"   {public_url.public_url}/health\")\n",
    "print(f\"   {public_url.public_url}/generate_audio\")\n",
    "print(f\"   {public_url.public_url}/generate_image\")\n",
    "print(f\"   {public_url.public_url}/generate_images_batch\")\n",
    "\n",
    "print(\"\\nğŸ”§ Update your backend config:\")\n",
    "print(f\"   KOKORO_API_URL = '{public_url.public_url}'\")\n",
    "print(f\"   SDXL_API_URL = '{public_url.public_url}'\")\n",
    "\n",
    "print(\"\\nğŸ’¡ Server will run until you stop this cell or disconnect from Colab\")\n",
    "print(\"â•\" * 80)\n",
    "\n",
    "# Keep server running\n",
    "try:\n",
    "    while True:\n",
    "        time.sleep(1)\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nğŸ›‘ Server stopped!\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}