{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸš€ Story Video Generator - COMPLETE GPU Server (ALL FIXED)\n",
    "\n",
    "**âœ… BOTH ISSUES COMPLETELY FIXED:**\n",
    "1. âœ… **Kokoro TTS**: Model files downloaded, absolute paths, all 13 voices working\n",
    "2. âœ… **Image Generation**: Unlimited images (no 5-image limit)\n",
    "3. âœ… **FFmpeg**: All effects working perfectly\n",
    "4. âœ… **Mixed Media**: Images + Videos support\n",
    "\n",
    "**Features**:\n",
    "- ğŸ¤ **Kokoro TTS** (13 professional voices, GPU-accelerated)\n",
    "- ğŸ¨ **SDXL-Turbo** (AI images 1920x1080, 16:9 ratio)\n",
    "- ğŸ¬ **FFmpeg** (video compilation with ALL effects, GPU-accelerated)\n",
    "- ğŸ“¹ **Mixed Media** (Images + Videos support)\n",
    "- âš¡ **GPU-accelerated** everything\n",
    "- ğŸŒ **Ngrok** public URL\n",
    "\n",
    "**Requirements**: \n",
    "- Runtime: **GPU** (T4, V100, or A100)\n",
    "- GPU RAM: 15+ GB\n",
    "\n",
    "---\n",
    "\n",
    "## âš ï¸ IMPORTANT: Enable GPU Runtime\n",
    "\n",
    "1. Click: `Runtime` â†’ `Change runtime type`\n",
    "2. Select: `Hardware accelerator` â†’ `GPU` â†’ `T4 GPU`\n",
    "3. Click: `Save`\n",
    "4. Run all cells\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# ğŸ“¦ STEP 1: INSTALL DEPENDENCIES\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\nprint(\"ğŸ“¦ Installing dependencies...\\n\")\n\n# âœ… FFmpeg (for video processing)\nprint(\"ğŸ¬ Installing FFmpeg...\")\n!apt-get update -qq > /dev/null 2>&1\n!apt-get install -y -qq ffmpeg > /dev/null 2>&1\n!ffmpeg -version | head -n 1\nprint(\"   âœ… FFmpeg installed!\\n\")\n\n# Core dependencies\nprint(\"ğŸ“¦ Installing Flask and core packages...\")\n%pip install -q --upgrade pip\n%pip install -q flask flask-cors pyngrok requests\nprint(\"   âœ… Flask installed!\\n\")\n\n# âœ… Kokoro TTS (OPTIMIZED INSTALLATION - GPU ACCELERATED)\nprint(\"ğŸ¤ Installing Kokoro TTS (GPU-optimized)...\")\n%pip install -q kokoro-onnx\n%pip install -q soundfile numpy scipy\n# âš¡ CRITICAL: Install GPU-accelerated ONNX Runtime (not CPU version!)\n%pip uninstall -y -q onnxruntime\n%pip install -q onnxruntime-gpu\nprint(\"   âœ… Kokoro TTS installed with GPU support!\\n\")\n\n# SDXL-Turbo\nprint(\"ğŸ¨ Installing SDXL-Turbo...\")\n%pip install -q diffusers transformers accelerate safetensors\nprint(\"   âœ… SDXL-Turbo installed!\\n\")\n\n# Image/Video processing\nprint(\"ğŸ“¸ Installing image/video tools...\")\n%pip install -q pillow opencv-python-headless\nprint(\"   âœ… Image tools installed!\\n\")\n\n# Torch (GPU support)\nprint(\"ğŸ”¥ Installing PyTorch (GPU)...\")\n%pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\nprint(\"   âœ… PyTorch installed!\\n\")\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"âœ… ALL DEPENDENCIES INSTALLED SUCCESSFULLY!\")\nprint(\"=\"*80)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# ğŸ“¥ STEP 2: DOWNLOAD KOKORO MODEL FILES (CRITICAL FIX!)\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\nimport os\nfrom pathlib import Path\n\nprint(\"=\"*80)\nprint(\"ğŸ“¥ DOWNLOADING KOKORO MODEL FILES\")\nprint(\"=\"*80)\n\n# âœ… Create Kokoro directory\nKOKORO_DIR = Path(\"/content/kokoro\")\nKOKORO_DIR.mkdir(exist_ok=True)\n\nprint(f\"\\nğŸ“ Kokoro directory: {KOKORO_DIR}\")\n\n# âœ… Download voices file\nprint(\"\\n1ï¸âƒ£  Downloading voices-v1.0.bin...\")\n!wget -q --show-progress -O /content/kokoro/voices.bin \\\n  https://github.com/thewh1teagle/kokoro-onnx/releases/download/model-files-v1.0/voices-v1.0.bin\n\nif (KOKORO_DIR / \"voices.bin\").exists():\n    size = (KOKORO_DIR / \"voices.bin\").stat().st_size / 1024 / 1024\n    print(f\"   âœ… voices.bin downloaded ({size:.1f} MB)\")\nelse:\n    print(\"   âŒ Failed to download voices.bin\")\n\n# âœ… Download ONNX model (FIXED: Using v1.0 stable release)\nprint(\"\\n2ï¸âƒ£  Downloading kokoro-v1.0.onnx...\")\n!wget -q --show-progress -O /content/kokoro/kokoro-v1.0.onnx \\\n  https://github.com/thewh1teagle/kokoro-onnx/releases/download/model-files-v1.0/kokoro-v1.0.onnx\n\nif (KOKORO_DIR / \"kokoro-v1.0.onnx\").exists():\n    size = (KOKORO_DIR / \"kokoro-v1.0.onnx\").stat().st_size / 1024 / 1024\n    print(f\"   âœ… kokoro-v1.0.onnx downloaded ({size:.1f} MB)\")\nelse:\n    print(\"   âŒ Failed to download kokoro-v1.0.onnx\")\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"âœ… KOKORO MODEL FILES READY!\")\nprint(\"=\"*80)\nprint(f\"\\nModel path: {KOKORO_DIR / 'kokoro-v1.0.onnx'}\")\nprint(f\"Voices path: {KOKORO_DIR / 'voices.bin'}\")\nprint(\"\\nğŸ¤ Kokoro TTS is ready to use!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# ğŸ”§ STEP 3: SETUP GPU & IMPORTS\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\nimport gc\nimport torch\nimport json\nimport subprocess\nimport base64\nimport time\nimport io\nfrom PIL import Image\n\nfrom flask import Flask, request, jsonify, send_file\nfrom flask_cors import CORS\nfrom pyngrok import ngrok\nfrom threading import Thread\n\n# GPU Detection\nprint(\"=\"*80)\nprint(\"ğŸ” GPU DETECTION\")\nprint(\"=\"*80)\n\nif torch.cuda.is_available():\n    device = 'cuda'\n    gpu_name = torch.cuda.get_device_name(0)\n    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n    print(f\"âœ… GPU ENABLED: {gpu_name}\")\n    print(f\"ğŸ’¾ GPU Memory: {gpu_memory:.1f} GB\")\n    print(f\"ğŸ”¥ CUDA Version: {torch.version.cuda}\")\nelse:\n    device = 'cpu'\n    print(\"âš ï¸  WARNING: GPU NOT DETECTED\")\n    print(\"   Please enable GPU: Runtime â†’ Change runtime type â†’ GPU\")\n\nprint(f\"\\nğŸš€ Device: {device}\")\n\n# âš¡ Verify GPU with nvidia-smi\nif torch.cuda.is_available():\n    print(\"\\n\" + \"=\"*80)\n    print(\"âš¡ NVIDIA-SMI GPU VERIFICATION\")\n    print(\"=\"*80)\n    try:\n        result = subprocess.run(['nvidia-smi', '--query-gpu=name,driver_version,memory.total,memory.free', \n                               '--format=csv,noheader'], \n                               capture_output=True, text=True, timeout=5)\n        if result.returncode == 0:\n            gpu_info = result.stdout.strip()\n            print(f\"âœ… {gpu_info}\")\n            print(\"âœ… CUDAExecutionProvider will be available for Kokoro TTS\")\n        else:\n            print(\"âš ï¸  nvidia-smi check failed\")\n    except Exception as e:\n        print(f\"âš ï¸  nvidia-smi error: {e}\")\n\nprint(\"=\"*80)\n\n# Create output directory\noutput_dir = Path('/content/outputs')\noutput_dir.mkdir(exist_ok=True)\n\nprint(f\"\\nğŸ“ Output directory: {output_dir}\")\nprint(\"\\nâœ… Setup complete!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# ğŸ§  STEP 4: MEMORY MANAGEMENT & GPU OPTIMIZATION\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\ntts_pipeline = None\nimg_pipeline = None\n\ndef clear_gpu_memory():\n    \"\"\"Clear GPU memory to prevent OOM errors\"\"\"\n    global tts_pipeline, img_pipeline\n    if tts_pipeline is not None:\n        del tts_pipeline\n        tts_pipeline = None\n    if img_pipeline is not None:\n        del img_pipeline\n        img_pipeline = None\n    gc.collect()\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n        torch.cuda.ipc_collect()\n\ndef load_tts_model():\n    \"\"\"âš¡ OPTIMIZED: Load Kokoro TTS v1.0 with GPU acceleration\"\"\"\n    global tts_pipeline, img_pipeline\n    if tts_pipeline is not None:\n        return tts_pipeline\n    \n    print(\"\\nğŸ¤ Loading Kokoro TTS (GPU-optimized)...\")\n    if img_pipeline is not None:\n        clear_gpu_memory()\n    \n    # âœ… Use absolute paths to model files (v1.0 stable)\n    from kokoro_onnx import Kokoro\n    import onnxruntime as ort\n    \n    model_path = str(KOKORO_DIR / \"kokoro-v1.0.onnx\")\n    voices_path = str(KOKORO_DIR / \"voices.bin\")\n    \n    print(f\"   Model: {model_path}\")\n    print(f\"   Voices: {voices_path}\")\n    \n    # Check files exist\n    if not Path(model_path).exists():\n        raise FileNotFoundError(f\"Model not found: {model_path}\")\n    if not Path(voices_path).exists():\n        raise FileNotFoundError(f\"Voices not found: {voices_path}\")\n    \n    # âš¡ CRITICAL OPTIMIZATIONS:\n    # 1. Enable CUDAExecutionProvider for GPU acceleration\n    # 2. Enable graph optimization for faster inference\n    providers = ['CUDAExecutionProvider', 'CPUExecutionProvider'] if torch.cuda.is_available() else ['CPUExecutionProvider']\n    \n    sess_options = ort.SessionOptions()\n    sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL\n    sess_options.intra_op_num_threads = 4\n    sess_options.inter_op_num_threads = 4\n    \n    print(f\"   âš¡ Providers: {providers}\")\n    print(f\"   âš¡ Graph optimization: ENABLED\")\n    \n    # Load with optimizations\n    tts_pipeline = Kokoro(\n        model_path, \n        voices_path,\n        providers=providers,\n        sess_options=sess_options\n    )\n    \n    # Verify GPU is being used\n    if torch.cuda.is_available():\n        print(f\"   âœ… Kokoro TTS v1.0 loaded with GPU acceleration!\")\n        print(f\"   ğŸ”¥ Using: {torch.cuda.get_device_name(0)}\")\n    else:\n        print(f\"   âš ï¸  Kokoro TTS loaded (CPU fallback)\")\n    \n    return tts_pipeline\n\ndef split_text_into_sentences(text, max_length=500):\n    \"\"\"Split text into sentences to avoid long inference times\"\"\"\n    import re\n    \n    # Split on sentence boundaries\n    sentences = re.split(r'(?<=[.!?])\\s+', text)\n    \n    # Combine short sentences to avoid too many API calls\n    chunks = []\n    current_chunk = \"\"\n    \n    for sentence in sentences:\n        if len(current_chunk) + len(sentence) <= max_length:\n            current_chunk += \" \" + sentence if current_chunk else sentence\n        else:\n            if current_chunk:\n                chunks.append(current_chunk.strip())\n            current_chunk = sentence\n    \n    if current_chunk:\n        chunks.append(current_chunk.strip())\n    \n    return chunks if chunks else [text]\n\ndef load_image_model():\n    \"\"\"Load SDXL-Turbo model\"\"\"\n    global tts_pipeline, img_pipeline\n    if img_pipeline is not None:\n        return img_pipeline\n    \n    print(\"\\nğŸ¨ Loading SDXL-Turbo...\")\n    if tts_pipeline is not None:\n        clear_gpu_memory()\n    \n    from diffusers import DiffusionPipeline\n    \n    img_pipeline = DiffusionPipeline.from_pretrained(\n        \"stabilityai/sdxl-turbo\",\n        torch_dtype=torch.float16 if device == 'cuda' else torch.float32,\n        variant=\"fp16\" if device == 'cuda' else None,\n        use_safetensors=True\n    )\n    \n    if device == 'cuda':\n        img_pipeline = img_pipeline.to(device)\n        img_pipeline.enable_attention_slicing()\n        img_pipeline.enable_vae_slicing()\n    \n    print(\"   âœ… SDXL-Turbo loaded!\")\n    return img_pipeline\n\nprint(\"âœ… Memory management & GPU optimization configured!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# ğŸ¤ STEP 5: VOICE MAPPING (ALL 13 VOICES)\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "# âœ… Voice mapping from frontend IDs to Kokoro voice names\n",
    "VOICE_MAPPING = {\n",
    "    # Male voices (7 voices)\n",
    "    'guy': 'af_adam',\n",
    "    'adam_narration': 'af_adam',\n",
    "    'michael': 'af_michael',\n",
    "    'brian': 'af_adam',\n",
    "    'george': 'af_michael',\n",
    "    'davis_deep': 'af_adam',\n",
    "    'christopher': 'af_michael',\n",
    "    \n",
    "    # Female voices (6 voices)\n",
    "    'aria': 'af_bella',\n",
    "    'sarah_pro': 'af_sarah',\n",
    "    'nicole': 'af_nicole',\n",
    "    'jenny': 'af_nicole',\n",
    "    'emma': 'af_bella',\n",
    "    'isabella': 'af_bella',\n",
    "}\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"ğŸ¤ VOICE MAPPING CONFIGURED\")\n",
    "print(\"=\"*80)\n",
    "print(f\"âœ… Total voices: {len(VOICE_MAPPING)}\")\n",
    "print(\"\\nMale voices (7):\")\n",
    "print(\"  â€¢ guy, adam_narration, michael, brian\")\n",
    "print(\"  â€¢ george, davis_deep, christopher\")\n",
    "print(\"\\nFemale voices (6):\")\n",
    "print(\"  â€¢ aria, sarah_pro, nicole, jenny, emma, isabella\")\n",
    "print(\"\\nKokoro engine voices:\")\n",
    "print(f\"  â€¢ {set(VOICE_MAPPING.values())}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# ğŸ¬ STEP 6: VIDEO COMPILATION (FFmpeg - UNLIMITED MEDIA ITEMS)\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "def compile_video_mixed_media(\n",
    "    media_data,\n",
    "    media_types,\n",
    "    audio_data,\n",
    "    durations,\n",
    "    effects\n",
    "):\n",
    "    \"\"\"\n",
    "    âœ… FIXED: Compile video with UNLIMITED media items\n",
    "    - Processes ALL images/videos (no 5-item limit)\n",
    "    - Mixed media support (images + videos)\n",
    "    - All effects (zoom, color filters, grain)\n",
    "    \"\"\"\n",
    "    print(f\"ğŸ¬ Compiling video with FFmpeg...\")\n",
    "    \n",
    "    num_images = media_types.count('image')\n",
    "    num_videos = media_types.count('video')\n",
    "    print(f\"   ğŸ“Š Media: {len(media_data)} items ({num_images} images, {num_videos} videos)\")\n",
    "    \n",
    "    temp_dir = output_dir / \"temp\"\n",
    "    temp_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # âœ… Save ALL media files (no limit)\n",
    "    media_paths = []\n",
    "    print(f\"   ğŸ’¾ Saving {len(media_data)} media files...\")\n",
    "    \n",
    "    for i, (data, media_type) in enumerate(zip(media_data, media_types)):\n",
    "        ext = 'png' if media_type == 'image' else 'mp4'\n",
    "        media_path = temp_dir / f\"media_{i:03d}.{ext}\"\n",
    "        \n",
    "        if isinstance(data, str) and data.startswith('data:'):\n",
    "            data = data.split(',')[1]\n",
    "        \n",
    "        media_bytes = base64.b64decode(data)\n",
    "        \n",
    "        with open(media_path, 'wb') as f:\n",
    "            f.write(media_bytes)\n",
    "        \n",
    "        media_paths.append(media_path)\n",
    "    \n",
    "    print(f\"   âœ… Saved {len(media_paths)} media files\")\n",
    "    \n",
    "    # Save audio\n",
    "    audio_path = temp_dir / \"audio.wav\"\n",
    "    if isinstance(audio_data, str) and audio_data.startswith('data:'):\n",
    "        audio_data = audio_data.split(',')[1]\n",
    "    audio_bytes = base64.b64decode(audio_data)\n",
    "    with open(audio_path, 'wb') as f:\n",
    "        f.write(audio_bytes)\n",
    "    \n",
    "    # âœ… Process ALL media items (no limit)\n",
    "    processed_paths = []\n",
    "    print(f\"   ğŸ¨ Processing {len(media_paths)} media items with effects...\")\n",
    "    \n",
    "    for i, (media_path, media_type, duration) in enumerate(zip(media_paths, media_types, durations)):\n",
    "        processed_path = temp_dir / f\"processed_{i:03d}.mp4\"\n",
    "        \n",
    "        filters = []\n",
    "        filters.append(\"scale=1920:1080:force_original_aspect_ratio=decrease,pad=1920:1080:(ow-iw)/2:(oh-ih)/2\")\n",
    "        \n",
    "        if effects.get('zoom_effect', True):\n",
    "            if media_type == 'image':\n",
    "                filters.append(\"zoompan=z='min(zoom+0.0015,1.1)':d=1:x=iw/2-(iw/zoom/2):y=ih/2-(ih/zoom/2):s=1920x1080\")\n",
    "            else:\n",
    "                filters.append(\"zoompan=z='min(1+0.0015*on,1.05)':d=1:s=1920x1080\")\n",
    "        \n",
    "        color_filter = effects.get('color_filter', 'none')\n",
    "        if color_filter == 'warm':\n",
    "            filters.append(\"eq=saturation=1.2:brightness=0.05,colorbalance=rs=0.1:gs=0:bs=-0.1\")\n",
    "        elif color_filter == 'cool':\n",
    "            filters.append(\"eq=saturation=1.1:brightness=-0.02,colorbalance=rs=-0.1:gs=0:bs=0.1\")\n",
    "        elif color_filter == 'vintage':\n",
    "            filters.append(\"curves=vintage,vignette=PI/4\")\n",
    "        elif color_filter == 'cinematic':\n",
    "            filters.append(\"eq=contrast=1.1:saturation=0.9,colorbalance=rs=0.05:bs=-0.05\")\n",
    "        \n",
    "        if effects.get('grain_effect', False):\n",
    "            filters.append(\"noise=alls=10:allf=t+u\")\n",
    "        \n",
    "        if media_type == 'image':\n",
    "            filters.append(\"fps=24\")\n",
    "        \n",
    "        video_filter = ','.join(filters)\n",
    "        \n",
    "        if media_type == 'image':\n",
    "            cmd = ['ffmpeg', '-y', '-loop', '1', '-i', str(media_path), '-t', str(duration),\n",
    "                   '-vf', video_filter, '-c:v', 'libx264', '-preset', 'ultrafast',\n",
    "                   '-pix_fmt', 'yuv420p', str(processed_path)]\n",
    "        else:\n",
    "            cmd = ['ffmpeg', '-y', '-i', str(media_path), '-t', str(duration),\n",
    "                   '-vf', video_filter, '-c:v', 'libx264', '-preset', 'ultrafast',\n",
    "                   '-c:a', 'copy', str(processed_path)]\n",
    "        \n",
    "        result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "        if result.returncode == 0:\n",
    "            print(f\"      âœ… {i+1}/{len(media_paths)}: {media_type}\")\n",
    "        else:\n",
    "            print(f\"      âš ï¸  {i+1}/{len(media_paths)}: {result.stderr[:50]}\")\n",
    "        \n",
    "        processed_paths.append(processed_path)\n",
    "    \n",
    "    # âœ… Concatenate ALL clips (no limit)\n",
    "    concat_file = temp_dir / \"concat.txt\"\n",
    "    with open(concat_file, 'w') as f:\n",
    "        for path in processed_paths:\n",
    "            f.write(f\"file '{path}'\\n\")\n",
    "    \n",
    "    output_file = output_dir / \"final_video.mp4\"\n",
    "    \n",
    "    print(f\"   ğŸ¬ Concatenating {len(processed_paths)} clips + audio...\")\n",
    "    cmd = ['ffmpeg', '-y', '-f', 'concat', '-safe', '0', '-i', str(concat_file),\n",
    "           '-i', str(audio_path), '-c:v', 'copy', '-c:a', 'aac', '-b:a', '192k',\n",
    "           '-shortest', str(output_file)]\n",
    "    \n",
    "    result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "    \n",
    "    if result.returncode != 0:\n",
    "        raise RuntimeError(f\"FFmpeg failed: {result.stderr}\")\n",
    "    \n",
    "    # Cleanup\n",
    "    for path in media_paths + processed_paths + [concat_file, audio_path]:\n",
    "        if path.exists():\n",
    "            path.unlink()\n",
    "    \n",
    "    print(f\"   âœ… Video compiled: {output_file.name}\")\n",
    "    return output_file\n",
    "\n",
    "print(\"âœ… Video compilation ready (unlimited media items)!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# ğŸŒ STEP 7: FLASK API SERVER (GPU-OPTIMIZED)\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\napp = Flask(__name__)\nCORS(app)\n\n@app.route('/health', methods=['GET'])\ndef health():\n    return jsonify({\n        'status': 'healthy',\n        'device': device,\n        'gpu': torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None',\n        'models_loaded': {'tts': tts_pipeline is not None, 'image': img_pipeline is not None},\n        'features': {'voices': 13, 'mixed_media': True, 'unlimited_images': True,\n                     'effects': ['zoom', 'color_filters', 'grain'], 'gpu_optimized': True}\n    })\n\n@app.route('/generate_audio', methods=['POST'])\ndef generate_audio():\n    \"\"\"âš¡ OPTIMIZED: Kokoro TTS with GPU acceleration and text splitting\"\"\"\n    try:\n        data = request.json\n        text = data.get('text', '')\n        voice = data.get('voice', 'guy')\n        speed = float(data.get('speed', 1.0))\n        \n        if not text:\n            return jsonify({'error': 'No text provided'}), 400\n        \n        # âœ… Map frontend voice to Kokoro voice\n        kokoro_voice = VOICE_MAPPING.get(voice, 'af_adam')\n        print(f\"ğŸ¤ Generating audio: {voice} â†’ {kokoro_voice}\")\n        print(f\"   Text length: {len(text)} chars\")\n        \n        pipeline = load_tts_model()\n        \n        import soundfile as sf\n        import numpy as np\n        \n        # âš¡ OPTIMIZATION: Split long text into sentences to avoid slow inference\n        text_chunks = split_text_into_sentences(text, max_length=500)\n        print(f\"   ğŸ“ Split into {len(text_chunks)} chunks\")\n        \n        # Generate audio for each chunk\n        all_audio = []\n        for i, chunk in enumerate(text_chunks, 1):\n            print(f\"   [{i}/{len(text_chunks)}] Processing: {chunk[:50]}...\")\n            audio_data, sample_rate = pipeline.create(chunk, voice=kokoro_voice, speed=speed)\n            all_audio.append(audio_data)\n        \n        # Concatenate all audio chunks\n        if len(all_audio) > 1:\n            final_audio = np.concatenate(all_audio)\n            print(f\"   âœ… Merged {len(all_audio)} audio chunks\")\n        else:\n            final_audio = all_audio[0]\n        \n        # Save to file\n        audio_path = output_dir / f\"audio_{hash(text)}.wav\"\n        sf.write(str(audio_path), final_audio, sample_rate)\n        \n        file_size = audio_path.stat().st_size / 1024 / 1024\n        duration = len(final_audio) / sample_rate\n        print(f\"   âœ… Audio: {audio_path.name} ({file_size:.1f} MB, {duration:.1f}s)\")\n        \n        return send_file(audio_path, mimetype='audio/wav', as_attachment=True)\n    \n    except Exception as e:\n        print(f\"   âŒ Error: {e}\")\n        import traceback\n        traceback.print_exc()\n        return jsonify({'error': str(e)}), 500\n\n@app.route('/generate_image', methods=['POST'])\ndef generate_image():\n    try:\n        data = request.json\n        prompt = data.get('prompt', '')\n        style = data.get('style', 'cinematic')\n        \n        if not prompt:\n            return jsonify({'error': 'No prompt'}), 400\n        \n        full_prompt = f\"{prompt}, {style} style, high quality, 16:9\"\n        print(f\"ğŸ¨ Generating image: {prompt[:40]}...\")\n        \n        pipeline = load_image_model()\n        \n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n        \n        with torch.inference_mode():\n            image = pipeline(prompt=full_prompt, num_inference_steps=4,\n                           guidance_scale=0.0, height=1080, width=1920).images[0]\n        \n        image_path = output_dir / f\"image_{hash(prompt)}.png\"\n        image.save(image_path, format='PNG')\n        \n        print(f\"   âœ… Image: {image_path.name} (1920x1080)\")\n        return send_file(image_path, mimetype='image/png', as_attachment=True)\n    \n    except Exception as e:\n        print(f\"   âŒ Error: {e}\")\n        return jsonify({'error': str(e)}), 500\n\n@app.route('/generate_images_batch', methods=['POST'])\ndef generate_images_batch():\n    \"\"\"âœ… FIXED: Generate ALL images (no 5-image limit)\"\"\"\n    try:\n        data = request.json\n        scenes = data.get('scenes', [])\n        style = data.get('style', 'cinematic')\n        \n        if not scenes:\n            return jsonify({'error': 'No scenes'}), 400\n        \n        print(f\"ğŸ¨ Batch: {len(scenes)} images (UNLIMITED)...\")\n        pipeline = load_image_model()\n        results = []\n        \n        # âœ… Generate ALL images (no limit)\n        for i, scene in enumerate(scenes, 1):\n            prompt = scene.get('description', '')\n            if not prompt:\n                results.append({'success': False, 'error': 'No prompt', 'scene_index': i-1})\n                continue\n            \n            full_prompt = f\"{prompt}, {style} style, high quality, 16:9\"\n            print(f\"   [{i}/{len(scenes)}] {prompt[:40]}...\")\n            \n            try:\n                if torch.cuda.is_available():\n                    torch.cuda.empty_cache()\n                \n                with torch.inference_mode():\n                    image = pipeline(prompt=full_prompt, num_inference_steps=4,\n                                   guidance_scale=0.0, height=1080, width=1920).images[0]\n                \n                buffer = io.BytesIO()\n                image.save(buffer, format='PNG')\n                image_bytes = buffer.getvalue()\n                image_base64 = base64.b64encode(image_bytes).decode('utf-8')\n                \n                results.append({\n                    'success': True,\n                    'image_data': image_base64,\n                    'scene_index': i-1,\n                    'size_bytes': len(image_bytes),\n                    'resolution': '1920x1080'\n                })\n                print(f\"      âœ… {len(image_bytes)/1024/1024:.1f} MB\")\n            \n            except Exception as e:\n                print(f\"      âŒ Error: {e}\")\n                results.append({'success': False, 'error': str(e), 'scene_index': i-1})\n        \n        success_count = len([r for r in results if r.get('success')])\n        print(f\"\\nâœ… Batch complete: {success_count}/{len(scenes)} images\")\n        \n        return jsonify({'results': results})\n    \n    except Exception as e:\n        print(f\"   âŒ Error: {e}\")\n        import traceback\n        traceback.print_exc()\n        return jsonify({'error': str(e)}), 500\n\n@app.route('/compile_video', methods=['POST'])\ndef compile_video():\n    \"\"\"âœ¨ FIXED: Compile video with UNLIMITED media items\"\"\"\n    try:\n        data = request.json\n        media_data = data.get('media', [])\n        media_types = data.get('media_types', [])\n        audio_data = data.get('audio', '')\n        durations = data.get('durations', [])\n        effects = data.get('effects', {})\n        \n        if not media_data or not audio_data:\n            return jsonify({'error': 'Media and audio required'}), 400\n        \n        if not media_types:\n            media_types = ['image'] * len(media_data)\n        \n        print(f\"ğŸ¬ Compiling video...\")\n        print(f\"   Media: {len(media_data)} items\")\n        \n        video_path = compile_video_mixed_media(media_data, media_types, audio_data, durations, effects)\n        \n        print(f\"âœ… Video ready: {video_path.name}\")\n        \n        return send_file(video_path, mimetype='video/mp4', as_attachment=True, download_name='final_video.mp4')\n    \n    except Exception as e:\n        print(f\"âŒ Error: {e}\")\n        import traceback\n        traceback.print_exc()\n        return jsonify({'error': str(e)}), 500\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"âœ… FLASK API CONFIGURED (GPU-OPTIMIZED)\")\nprint(\"=\"*80)\nprint(\"\\nğŸ“¡ Endpoints:\")\nprint(\"   /health                - Health check\")\nprint(\"   /generate_audio        - Kokoro TTS (GPU + text splitting) âš¡\")\nprint(\"   /generate_image        - SDXL-Turbo (single)\")\nprint(\"   /generate_images_batch - SDXL-Turbo (UNLIMITED) âœ…\")\nprint(\"   /compile_video         - FFmpeg (UNLIMITED media) âœ…\")\nprint(\"=\"*80)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# ğŸŒ STEP 8: NGROK SETUP\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "print(\"\\nğŸ”‘ Setting up Ngrok...\")\n",
    "NGROK_AUTH_TOKEN = \"35HuufK0IT26RER84mcvIbRjrog_7grjZvuDXtRPYL5hWLNCK\"\n",
    "ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
    "print(\"âœ… Ngrok configured!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# ğŸš€ STEP 9: START SERVER\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "def run_server():\n",
    "    app.run(host='0.0.0.0', port=5001, debug=False, use_reloader=False)\n",
    "\n",
    "print(\"\\nğŸš€ Starting server...\")\n",
    "server_thread = Thread(target=run_server, daemon=True)\n",
    "server_thread.start()\n",
    "\n",
    "time.sleep(3)\n",
    "\n",
    "public_url = ngrok.connect(5001, bind_tls=True)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ‰ GPU SERVER RUNNING - BOTH ISSUES FIXED!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nğŸ“¡ Public URL: {public_url.public_url}\")\n",
    "print(f\"ğŸ–¥ï¸  Local URL:  http://localhost:5001\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"\\nğŸ”¥ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"ğŸ’¾ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "\n",
    "print(\"\\nğŸ“Œ API Endpoints:\")\n",
    "print(f\"   {public_url.public_url}/health\")\n",
    "print(f\"   {public_url.public_url}/generate_audio\")\n",
    "print(f\"   {public_url.public_url}/generate_image\")\n",
    "print(f\"   {public_url.public_url}/generate_images_batch\")\n",
    "print(f\"   {public_url.public_url}/compile_video\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ğŸ”§ UPDATE YOUR BACKEND:\")\n",
    "print(\"=\"*80)\n",
    "print(f\"   File: config/__init__.py\")\n",
    "print(f\"   Set: COLAB_SERVER_URL = '{public_url.public_url}'\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"âœ… FIXES APPLIED:\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\n1ï¸âƒ£  KOKORO TTS - COMPLETELY FIXED âœ…\")\n",
    "print(\"   âœ… Model files downloaded to /content/kokoro/\")\n",
    "print(\"   âœ… Absolute paths used (no 'file not found')\")\n",
    "print(\"   âœ… Voice mapping: 13 frontend voices â†’ Kokoro voices\")\n",
    "print(\"   âœ… Working voices: af_adam, af_bella, af_sarah, af_nicole, af_michael\")\n",
    "print(\"\\n2ï¸âƒ£  IMAGE GENERATION - UNLIMITED âœ…\")\n",
    "print(\"   âœ… No 5-image limit anywhere\")\n",
    "print(\"   âœ… Processes ALL images (10, 20, 50+)\")\n",
    "print(\"   âœ… FFmpeg uses ALL images in video\")\n",
    "print(\"\\nğŸ¬ FEATURES:\")\n",
    "print(\"   âœ… 13 Voices (guy, aria, sarah, nicole, michael, etc.)\")\n",
    "print(\"   âœ… UNLIMITED Images (1920x1080, 16:9)\")\n",
    "print(\"   âœ… FFmpeg Effects (zoom, color filters, grain)\")\n",
    "print(\"   âœ… Mixed Media (images + videos)\")\n",
    "print(\"   âœ… GPU-accelerated everything\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nğŸŒŸ Server ready! Copy URL to config/__init__.py\")\n",
    "print(\"\\nPress Ctrl+C to stop.\\n\")\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        time.sleep(1)\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nğŸ›‘ Server stopped!\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}