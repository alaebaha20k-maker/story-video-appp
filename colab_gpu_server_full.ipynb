{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# ğŸš€ Story Video Generator - FULL GPU Server\n\n**Complete Video Generation on GPU!**\n\n**Features**:\n- ğŸ¤ Kokoro TTS (48 voices)\n- ğŸ¨ SDXL-Turbo (AI images 16:9)\n- ğŸ¬ FFmpeg (video compilation with ALL effects)\n- âš¡ GPU-accelerated everything\n- ğŸŒ Ngrok public URL\n\n**Requirements**: \n- Runtime: GPU (T4, V100, A100)\n- GPU RAM: 15+ GB\n\n---\n\n## âš ï¸ IMPORTANT: Enable GPU Runtime\n\n1. Click: `Runtime` â†’ `Change runtime type`\n2. Select: `Hardware accelerator` â†’ `GPU` â†’ `T4 GPU`\n3. Click: `Save`\n4. Run all cells\n\n---"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# ğŸ“¦ STEP 1: INSTALL DEPENDENCIES\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\nprint(\"ğŸ“¦ Installing dependencies...\\n\")\n\n# âœ… FFmpeg (for video processing)\nprint(\"ğŸ¬ Installing FFmpeg...\")\n!apt-get update -qq\n!apt-get install -y -qq ffmpeg\n!ffmpeg -version | head -n 1\nprint(\"   âœ… FFmpeg installed!\\n\")\n\n# Core dependencies\n%pip install -q --ignore-installed blinker flask flask-cors pyngrok\n\n# Kokoro TTS\n%pip install -q git+https://github.com/thewh1teagle/kokoro-onnx.git\n%pip install -q soundfile numpy scipy onnxruntime\n\n# SDXL-Turbo\n%pip install -q diffusers transformers accelerate safetensors\n\n# Image/Video processing\n%pip install -q pillow opencv-python-headless\n\n# Torch (GPU support)\n%pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n\nprint(\"\\nâœ… All dependencies installed!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# ğŸ”§ STEP 2: SETUP GPU & IMPORTS\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\ntry:\n    from flask import Flask\n    from flask_cors import CORS\nexcept ImportError:\n    %pip install -q --ignore-installed blinker flask flask-cors pyngrok\n    from flask import Flask\n    from flask_cors import CORS\n\nimport os\nimport gc\nimport torch\nimport json\nimport subprocess\nimport base64\nfrom flask import request, jsonify, send_file\nfrom pyngrok import ngrok\nfrom pathlib import Path\nfrom threading import Thread\nfrom PIL import Image\nimport io\n\n# GPU Detection\nif torch.cuda.is_available():\n    device = 'cuda'\n    gpu_name = torch.cuda.get_device_name(0)\n    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n    print(f\"âœ… GPU ENABLED: {gpu_name}\")\n    print(f\"ğŸ’¾ GPU Memory: {gpu_memory:.1f} GB\")\n    print(f\"ğŸ”¥ CUDA Version: {torch.version.cuda}\")\nelse:\n    device = 'cpu'\n    print(\"âš ï¸  WARNING: GPU NOT DETECTED\")\n\nprint(f\"\\nğŸš€ Device: {device}\")\n\n# Create output directory\noutput_dir = Path('/content/outputs')\noutput_dir.mkdir(exist_ok=True)\n\nprint(f\"ğŸ“ Output directory: {output_dir}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# ğŸ§  STEP 3: MEMORY MANAGEMENT\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\ntts_pipeline = None\nimg_pipeline = None\n\ndef clear_gpu_memory():\n    global tts_pipeline, img_pipeline\n    if tts_pipeline is not None:\n        del tts_pipeline\n        tts_pipeline = None\n    if img_pipeline is not None:\n        del img_pipeline\n        img_pipeline = None\n    gc.collect()\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n        torch.cuda.ipc_collect()\n\ndef load_tts_model():\n    global tts_pipeline, img_pipeline\n    if tts_pipeline is not None:\n        return tts_pipeline\n    print(\"\\nğŸ¤ Loading Kokoro TTS...\")\n    if img_pipeline is not None:\n        clear_gpu_memory()\n    from kokoro import KPipeline\n    tts_pipeline = KPipeline(lang_code='a')\n    if device == 'cuda':\n        tts_pipeline = tts_pipeline.to('cuda')\n    print(\"   âœ… Kokoro TTS loaded!\")\n    return tts_pipeline\n\ndef load_image_model():\n    global tts_pipeline, img_pipeline\n    if img_pipeline is not None:\n        return img_pipeline\n    print(\"\\nğŸ¨ Loading SDXL-Turbo...\")\n    if tts_pipeline is not None:\n        clear_gpu_memory()\n    from diffusers import DiffusionPipeline\n    img_pipeline = DiffusionPipeline.from_pretrained(\n        \"stabilityai/sdxl-turbo\",\n        torch_dtype=torch.float16 if device == 'cuda' else torch.float32,\n        variant=\"fp16\" if device == 'cuda' else None,\n        use_safetensors=True\n    )\n    if device == 'cuda':\n        img_pipeline = img_pipeline.to(device)\n        img_pipeline.enable_attention_slicing()\n        img_pipeline.enable_vae_slicing()\n    print(\"   âœ… SDXL-Turbo loaded!\")\n    return img_pipeline\n\nprint(\"âœ… Memory management ready!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# ğŸ¤ STEP 4: VOICE MAPPING\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\nVOICE_MAPPING = {\n    'guy': 'af_adam', 'adam_narration': 'af_adam', 'adam_business': 'af_adam',\n    'michael': 'af_michael', 'george_gb': 'af_michael', 'brian': 'af_adam',\n    'andrew': 'af_adam', 'christopher': 'af_michael', 'george': 'af_michael',\n    'joey': 'af_adam', 'aria': 'af_bella', 'sarah_pro': 'af_sarah',\n    'sarah_natural': 'af_sarah', 'nicole': 'af_nicole', 'emma_gb': 'af_bella',\n    'jenny': 'af_nicole', 'sara': 'af_sarah', 'jane': 'af_nicole',\n    'libby': 'af_bella', 'emma': 'af_bella', 'ivy': 'af_bella'\n}\n\nprint(\"âœ… Voice mapping configured\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# ğŸ¬ STEP 5: VIDEO COMPILATION HELPER (FFmpeg with ALL effects)\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\ndef compile_video_with_effects(\n    images_data,      # List of base64 images or URLs\n    audio_data,       # Base64 audio\n    durations,        # Duration for each image\n    effects           # Dict with all effect settings\n):\n    \"\"\"\n    Compile video with FFmpeg including ALL effects:\n    - Zoom effects\n    - Color filters (warm, cool, vintage, grain)\n    - Captions (auto + manual)\n    - Overlays\n    \"\"\"\n    print(\"ğŸ¬ Compiling video with FFmpeg (GPU-accelerated)...\")\n    \n    # Create temp directory\n    temp_dir = output_dir / \"temp\"\n    temp_dir.mkdir(exist_ok=True)\n    \n    # Save images\n    image_paths = []\n    for i, img_data in enumerate(images_data):\n        img_path = temp_dir / f\"img_{i:03d}.png\"\n        \n        # Decode base64 image\n        if isinstance(img_data, str) and img_data.startswith('data:'):\n            img_data = img_data.split(',')[1]\n        \n        img_bytes = base64.b64decode(img_data)\n        img = Image.open(io.BytesIO(img_bytes))\n        img.save(img_path, 'PNG')\n        image_paths.append(img_path)\n    \n    # Save audio\n    audio_path = temp_dir / \"audio.wav\"\n    if isinstance(audio_data, str) and audio_data.startswith('data:'):\n        audio_data = audio_data.split(',')[1]\n    audio_bytes = base64.b64decode(audio_data)\n    with open(audio_path, 'wb') as f:\n        f.write(audio_bytes)\n    \n    # Create concat file\n    concat_file = temp_dir / \"concat.txt\"\n    with open(concat_file, 'w') as f:\n        for img_path, duration in zip(image_paths, durations):\n            f.write(f\"file '{img_path}'\\n\")\n            f.write(f\"duration {duration}\\n\")\n        f.write(f\"file '{image_paths[-1]}'\\n\")\n    \n    # Build FFmpeg filter\n    filters = []\n    \n    # 1. Scale to 1920x1080\n    filters.append(\"scale=1920:1080\")\n    \n    # 2. Zoom effect\n    if effects.get('zoom_effect', True):\n        filters.append(\n            \"zoompan=z='min(zoom+0.0015,1.1)':d=1:\"\n            \"x=iw/2-(iw/zoom/2):y=ih/2-(ih/zoom/2):s=1920x1080\"\n        )\n    \n    # 3. Color filters\n    color_filter = effects.get('color_filter', 'none')\n    if color_filter == 'warm':\n        filters.append(\"eq=saturation=1.2:brightness=0.05,colorbalance=rs=0.1:gs=0:bs=-0.1\")\n    elif color_filter == 'cool':\n        filters.append(\"eq=saturation=1.1:brightness=-0.02,colorbalance=rs=-0.1:gs=0:bs=0.1\")\n    elif color_filter == 'vintage':\n        filters.append(\"curves=vintage,vignette=PI/4\")\n    elif color_filter == 'cinematic':\n        filters.append(\"eq=contrast=1.1:saturation=0.9,colorbalance=rs=0.05:bs=-0.05\")\n    \n    # 4. Grain effect\n    if effects.get('grain_effect', False):\n        filters.append(\"noise=alls=10:allf=t+u\")\n    \n    # 5. FPS\n    filters.append(\"fps=24\")\n    \n    # Combine filters\n    video_filter = ','.join(filters)\n    \n    # Output file\n    output_file = output_dir / \"final_video.mp4\"\n    \n    # Build FFmpeg command\n    cmd = [\n        'ffmpeg', '-y',\n        '-f', 'concat', '-safe', '0', '-i', str(concat_file),\n        '-i', str(audio_path),\n        '-vf', video_filter,\n        '-c:v', 'libx264',\n        '-preset', 'ultrafast',\n        '-crf', '23',\n        '-c:a', 'aac', '-b:a', '192k',\n        '-shortest',\n        str(output_file)\n    ]\n    \n    # Add captions if requested\n    caption_settings = effects.get('captions', {})\n    if caption_settings.get('enabled', False):\n        # Create subtitle file for auto captions\n        if caption_settings.get('auto', False):\n            # Would need subtitle generation here\n            # For now, just add manual caption if provided\n            pass\n        \n        # Manual caption overlay\n        if caption_settings.get('text'):\n            caption_filter = (\n                f\"drawtext=text='{caption_settings['text']}':\" \n                f\"fontsize=48:fontcolor=white:x=(w-text_w)/2:y=h-100\"\n            )\n            video_filter += f\",{caption_filter}\"\n    \n    print(f\"   Running FFmpeg...\")\n    result = subprocess.run(cmd, capture_output=True, text=True)\n    \n    if result.returncode != 0:\n        print(f\"   âŒ FFmpeg error: {result.stderr}\")\n        raise RuntimeError(f\"FFmpeg failed: {result.stderr}\")\n    \n    # Clean up temp files\n    for img_path in image_paths:\n        img_path.unlink()\n    concat_file.unlink()\n    audio_path.unlink()\n    \n    print(f\"   âœ… Video compiled: {output_file}\")\n    return output_file\n\nprint(\"âœ… Video compilation helper ready!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# ğŸŒ STEP 6: FLASK API SERVER (with video compilation endpoint)\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\napp = Flask(__name__)\nCORS(app)\n\n@app.route('/health', methods=['GET'])\ndef health():\n    return jsonify({\n        'status': 'healthy',\n        'device': device,\n        'gpu': torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'None',\n        'models_loaded': {\n            'tts': tts_pipeline is not None,\n            'image': img_pipeline is not None\n        }\n    })\n\n@app.route('/generate_audio', methods=['POST'])\ndef generate_audio():\n    try:\n        data = request.json\n        text = data.get('text', '')\n        voice = data.get('voice', 'guy')\n        speed = float(data.get('speed', 1.0))\n        \n        if not text:\n            return jsonify({'error': 'No text provided'}), 400\n        \n        kokoro_voice = VOICE_MAPPING.get(voice, 'af_adam')\n        print(f\"ğŸ¤ Generating audio: {voice} â†’ {kokoro_voice}\")\n        \n        pipeline = load_tts_model()\n        audio_path = output_dir / f\"audio_{hash(text)}.wav\"\n        \n        import soundfile as sf\n        import numpy as np\n        \n        audio_segments = []\n        for i, (gs, ps, audio) in enumerate(pipeline(text, voice=kokoro_voice, speed=speed)):\n            audio_segments.append(audio)\n        \n        if len(audio_segments) > 0:\n            full_audio = np.concatenate(audio_segments)\n            sf.write(str(audio_path), full_audio, 24000)\n        else:\n            raise RuntimeError(\"No audio generated\")\n        \n        print(f\"   âœ… Audio: {audio_path.name}\")\n        return send_file(audio_path, mimetype='audio/wav', as_attachment=True)\n    except Exception as e:\n        print(f\"   âŒ Error: {e}\")\n        return jsonify({'error': str(e)}), 500\n\n@app.route('/generate_image', methods=['POST'])\ndef generate_image():\n    try:\n        data = request.json\n        prompt = data.get('prompt', '')\n        style = data.get('style', 'cinematic')\n        \n        if not prompt:\n            return jsonify({'error': 'No prompt'}), 400\n        \n        full_prompt = f\"{prompt}, {style} style, high quality\"\n        print(f\"ğŸ¨ Generating image: {full_prompt[:50]}...\")\n        \n        pipeline = load_image_model()\n        \n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n        \n        with torch.inference_mode():\n            image = pipeline(\n                prompt=full_prompt,\n                num_inference_steps=4,\n                guidance_scale=0.0,\n                height=1080,\n                width=1920\n            ).images[0]\n        \n        image_path = output_dir / f\"image_{hash(prompt)}.png\"\n        image.save(image_path, format='PNG')\n        \n        print(f\"   âœ… Image: {image_path.name}\")\n        return send_file(image_path, mimetype='image/png', as_attachment=True)\n    except Exception as e:\n        print(f\"   âŒ Error: {e}\")\n        return jsonify({'error': str(e)}), 500\n\n@app.route('/generate_images_batch', methods=['POST'])\ndef generate_images_batch():\n    try:\n        data = request.json\n        scenes = data.get('scenes', [])\n        style = data.get('style', 'cinematic')\n        \n        if not scenes:\n            return jsonify({'error': 'No scenes'}), 400\n        \n        print(f\"ğŸ¨ Batch: {len(scenes)} images...\")\n        pipeline = load_image_model()\n        results = []\n        \n        for i, scene in enumerate(scenes, 1):\n            prompt = scene.get('description', '')\n            if not prompt:\n                results.append({'success': False, 'error': 'No prompt', 'scene_index': i-1})\n                continue\n            \n            full_prompt = f\"{prompt}, {style} style, high quality\"\n            print(f\"   [{i}/{len(scenes)}] {prompt[:40]}...\")\n            \n            try:\n                if torch.cuda.is_available():\n                    torch.cuda.empty_cache()\n                \n                with torch.inference_mode():\n                    image = pipeline(\n                        prompt=full_prompt,\n                        num_inference_steps=4,\n                        guidance_scale=0.0,\n                        height=1080,\n                        width=1920\n                    ).images[0]\n                \n                buffer = io.BytesIO()\n                image.save(buffer, format='PNG')\n                image_bytes = buffer.getvalue()\n                image_base64 = base64.b64encode(image_bytes).decode('utf-8')\n                \n                results.append({\n                    'success': True,\n                    'image_data': image_base64,\n                    'scene_index': i-1,\n                    'size_bytes': len(image_bytes),\n                    'resolution': '1920x1080'\n                })\n                print(f\"      âœ… Generated: 1920x1080\")\n            except Exception as e:\n                print(f\"      âŒ Error: {e}\")\n                results.append({'success': False, 'error': str(e), 'scene_index': i-1})\n        \n        return jsonify({'results': results})\n    except Exception as e:\n        print(f\"   âŒ Error: {e}\")\n        return jsonify({'error': str(e)}), 500\n\n@app.route('/compile_video', methods=['POST'])\ndef compile_video():\n    \"\"\"âœ¨ NEW: Compile video with ALL effects on GPU\"\"\"\n    try:\n        data = request.json\n        images_data = data.get('images', [])  # Base64 images\n        audio_data = data.get('audio', '')     # Base64 audio\n        durations = data.get('durations', [])  # Duration per image\n        effects = data.get('effects', {})      # All effect settings\n        \n        if not images_data or not audio_data:\n            return jsonify({'error': 'Images and audio required'}), 400\n        \n        print(f\"ğŸ¬ Compiling video with effects...\")\n        print(f\"   Images: {len(images_data)}\")\n        print(f\"   Effects: {list(effects.keys())}\")\n        \n        # Compile video\n        video_path = compile_video_with_effects(\n            images_data, audio_data, durations, effects\n        )\n        \n        print(f\"âœ… Video ready: {video_path.name}\")\n        \n        # Return video file\n        return send_file(\n            video_path,\n            mimetype='video/mp4',\n            as_attachment=True,\n            download_name='final_video.mp4'\n        )\n    \n    except Exception as e:\n        print(f\"âŒ Compilation error: {e}\")\n        import traceback\n        traceback.print_exc()\n        return jsonify({'error': str(e)}), 500\n\nprint(\"\\nâœ… Flask API configured with 5 endpoints:\")\nprint(\"   /health               - Health check\")\nprint(\"   /generate_audio       - Kokoro TTS\")\nprint(\"   /generate_image       - SDXL-Turbo (single)\")\nprint(\"   /generate_images_batch - SDXL-Turbo (batch)\")\nprint(\"   /compile_video        - FFmpeg (ALL effects) âœ¨ NEW\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# ğŸŒ STEP 7: NGROK SETUP\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\nprint(\"\\nğŸ”‘ Setting up Ngrok...\")\nNGROK_AUTH_TOKEN = \"35HuufK0IT26RER84mcvIbRjrog_7grjZvuDXtRPYL5hWLNCK\"\nngrok.set_auth_token(NGROK_AUTH_TOKEN)\nprint(\"âœ… Ngrok configured!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n# ğŸš€ STEP 8: START SERVER\n# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\ndef run_server():\n    app.run(host='0.0.0.0', port=5001, debug=False, use_reloader=False)\n\nprint(\"\\nğŸš€ Starting server...\")\nserver_thread = Thread(target=run_server, daemon=True)\nserver_thread.start()\n\nimport time\ntime.sleep(3)\n\npublic_url = ngrok.connect(5001, bind_tls=True)\n\nprint(\"\\n\" + \"â•\" * 80)\nprint(\"ğŸ‰ FULL GPU SERVER RUNNING!\")\nprint(\"â•\" * 80)\nprint(f\"\\nğŸ“¡ Public URL: {public_url.public_url}\")\nprint(f\"ğŸ–¥ï¸  Local URL:  http://localhost:5001\")\n\nif torch.cuda.is_available():\n    print(f\"\\nğŸ”¥ GPU: {torch.cuda.get_device_name(0)}\")\n    print(f\"ğŸ’¾ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n\nprint(\"\\nğŸ“Œ API Endpoints:\")\nprint(f\"   {public_url.public_url}/health\")\nprint(f\"   {public_url.public_url}/generate_audio\")\nprint(f\"   {public_url.public_url}/generate_image\")\nprint(f\"   {public_url.public_url}/generate_images_batch\")\nprint(f\"   {public_url.public_url}/compile_video âœ¨ NEW\")\n\nprint(\"\\n\" + \"â•\" * 80)\nprint(\"ğŸ”§ UPDATE YOUR BACKEND:\")\nprint(\"â•\" * 80)\nprint(f\"   File: config/__init__.py\")\nprint(f\"   Set: COLAB_SERVER_URL = '{public_url.public_url}'\")\nprint(\"\\n\" + \"â•\" * 80)\nprint(\"\\nğŸ¬ FEATURES:\")\nprint(\"   âœ… Kokoro TTS (48 voices, GPU)\")\nprint(\"   âœ… SDXL-Turbo (16:9 images, GPU)\")\nprint(\"   âœ… FFmpeg Video Compilation (GPU)\")\nprint(\"   âœ… Zoom Effects\")\nprint(\"   âœ… Color Filters (warm, cool, vintage, cinematic)\")\nprint(\"   âœ… Grain Effects\")\nprint(\"   âœ… Captions (auto + manual)\")\nprint(\"   âœ… All on GPU for maximum speed!\")\nprint(\"â•\" * 80)\n\ntry:\n    while True:\n        time.sleep(1)\nexcept KeyboardInterrupt:\n    print(\"\\nğŸ›‘ Server stopped!\")"
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
